{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GcZ4Ec_bU0r"
      },
      "outputs": [],
      "source": [
        "from utils import train, test, LpLoss, get_filter, UnitGaussianNormalizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data_utils\n",
        "from typing import List, Tuple\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.io import loadmat, savemat\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import eval_legendre\n",
        "from sympy import Poly, legendre, Symbol\n",
        "import h5py\n",
        "\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu9qtnA-bDJN"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coupled Data\n",
        "\n",
        "ntrain = 1000\n",
        "ntest = 200\n",
        "\n",
        "sub = 2**0 #subsampling rate\n",
        "h = 2**8 // sub #total grid size divided by the subsampling rate\n",
        "s = h\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "rw_u = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Drho_t0_1.mat')\n",
        "x_data = rw_u['rho_t0'].astype(np.float32)\n",
        "y_data = rw_u['rho_t02'].astype(np.float32)\n",
        "\n",
        "x_train_u = x_data[:ntrain,::sub]\n",
        "y_train_u = y_data[:ntrain,::sub]\n",
        "x_test_u = x_data[-ntest:,::sub]\n",
        "y_test_u = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_u = torch.from_numpy(x_train_u)\n",
        "x_test_u = torch.from_numpy(x_test_u)\n",
        "y_train_u = torch.from_numpy(y_train_u)\n",
        "y_test_u = torch.from_numpy(y_test_u)\n",
        "\n",
        "x_train_u = x_train_u.unsqueeze(-1)\n",
        "x_test_u = x_test_u.unsqueeze(-1)\n",
        "\n",
        "\n",
        "rw_v = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Dphi_t0_1.mat')\n",
        "x_data = rw_v['phi_t0'].astype(np.float32)\n",
        "y_data = rw_v['phi_t02'].astype(np.float32)\n",
        "\n",
        "x_train_v = x_data[:ntrain,::sub]\n",
        "y_train_v = y_data[:ntrain,::sub]\n",
        "x_test_v = x_data[-ntest:,::sub]\n",
        "y_test_v = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_v = torch.from_numpy(x_train_v)\n",
        "x_test_v = torch.from_numpy(x_test_v)\n",
        "y_train_v = torch.from_numpy(y_train_v)\n",
        "y_test_v = torch.from_numpy(y_test_v)\n",
        "\n",
        "x_train_v = x_train_v.unsqueeze(-1)\n",
        "x_test_v = x_test_v.unsqueeze(-1)\n",
        "\n",
        "x_train = torch.cat([x_train_u.reshape(ntrain,s,-1), x_train_v.reshape(ntrain,s,-1)], dim=-1)\n",
        "x_test = torch.cat([x_test_u.reshape(ntest,s,-1), x_test_v.reshape(ntest,s,-1)], dim=-1)\n",
        "\n",
        "y_train = torch.cat([y_train_u.reshape(ntrain,s,-1), y_train_v.reshape(ntrain,s,-1)], dim=-1)\n",
        "y_test = torch.cat([y_test_u.reshape(ntest,s,-1), y_test_v.reshape(ntest,s,-1)], dim=-1)\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn7yKvqgcKMA"
      },
      "outputs": [],
      "source": [
        "def get_initializer(name):\n",
        "    \n",
        "    if name == 'xavier_normal':\n",
        "        init_ = partial(nn.init.xavier_normal_)\n",
        "    elif name == 'kaiming_uniform':\n",
        "        init_ = partial(nn.init.kaiming_uniform_)\n",
        "    elif name == 'kaiming_normal':\n",
        "        init_ = partial(nn.init.kaiming_normal_)\n",
        "    return init_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzysmfscb8AB"
      },
      "outputs": [],
      "source": [
        "class sparseKernel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1,\n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernel,self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.Li = nn.Linear(c*k, 128)\n",
        "        self.conv = self.convBlock(c*k, 128)\n",
        "#         self.Lo = nn.Linear(alpha*k, c*k)\n",
        "        self.Lo = nn.Linear(128, c*k)\n",
        "       \n",
        "    def forward(self, x):\n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        x = x.view(B, N, -1)\n",
        "#         x = F.relu(self.Li(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.Lo(x)\n",
        "        x = x.view(B, N, c, ich)\n",
        "       \n",
        "        return x\n",
        "       \n",
        "       \n",
        "    def convBlock(self, ich, och):\n",
        "        net = nn.Sequential(\n",
        "            nn.Conv1d(ich, och, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(och, och, 3, 1, 1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "        )\n",
        "        return net\n",
        "\n",
        "def compl_mul1d(x, weights):\n",
        "    # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "    return torch.einsum(\"bix,iox->box\", x, weights)\n",
        "\n",
        "class sparseKernelFT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1,\n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernelFT, self).__init__()       \n",
        "       \n",
        "        self.modes1 = alpha\n",
        "        self.scale = (1 / (c*k*c*k))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(c*k, c*k, self.modes1, dtype=torch.cfloat))\n",
        "        self.weights1.requires_grad = True\n",
        "        self.k = k\n",
        "       \n",
        "    def forward(self, x):\n",
        "        B, N, c, k = x.shape # (B, N, c, k)\n",
        "       \n",
        "        x = x.view(B, N, -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x_fft = torch.fft.rfft(x)\n",
        "        # Multiply relevant Fourier modes\n",
        "        l = min(self.modes1, N//2+1)\n",
        "        out_ft = torch.zeros(B, c*k, N//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
        "       \n",
        "        out_ft[:, :, :l] = compl_mul1d(x_fft[:, :, :l], self.weights1[:, :, :l])\n",
        "       \n",
        "\n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft(out_ft, n=N)\n",
        "        x = x.permute(0, 2, 1).view(B, N, c, k)\n",
        "        return x\n",
        "       \n",
        "   \n",
        "class MWT_CZ(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k = 3, alpha = 5,\n",
        "                 L = 0, c = 1,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT_CZ, self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.L = L\n",
        "        H0, H1, G0, G1 = get_filter(base, k)\n",
        "       \n",
        "        self.A = sparseKernelFT(k, alpha, c)\n",
        "        self.B = sparseKernelFT(k, alpha, c)\n",
        "        self.C = sparseKernelFT(k, alpha, c)\n",
        "       \n",
        "        self.T0 = nn.Linear(k, k)\n",
        "\n",
        "        self.register_buffer('ec_s', torch.Tensor(\n",
        "            np.concatenate((H0.T, H1.T), axis=0)))\n",
        "        self.register_buffer('ec_d', torch.Tensor(\n",
        "            np.concatenate((G0.T, G1.T), axis=0)))\n",
        "       \n",
        "        self.register_buffer('rc_e', torch.Tensor(\n",
        "            np.concatenate((H0, G0), axis=0)))\n",
        "        self.register_buffer('rc_o', torch.Tensor(\n",
        "            np.concatenate((H1, G1), axis=0)))\n",
        "       \n",
        "       \n",
        "    def forward(self, x):\n",
        "       \n",
        "        B, N, c, ich = x.shape # (B, N, k)\n",
        "        ns = math.floor(np.log2(N))\n",
        "\n",
        "        Ud = torch.jit.annotate(List[Tensor], [])\n",
        "        Us = torch.jit.annotate(List[Tensor], [])\n",
        "#         decompose\n",
        "        for i in range(ns-self.L):\n",
        "            d, x = self.wavelet_transform(x)\n",
        "            Ud += [self.A(d) + self.B(x)]\n",
        "            Us += [self.C(d)]\n",
        "        x = self.T0(x) # coarsest scale transform\n",
        "\n",
        "#        reconstruct           \n",
        "        for i in range(ns-1-self.L,-1,-1):\n",
        "            x = x + Us[i]\n",
        "            x = torch.cat((x, Ud[i]), -1)\n",
        "            x = self.evenOdd(x)\n",
        "        return x\n",
        "\n",
        "   \n",
        "    def wavelet_transform(self, x):\n",
        "        xa = torch.cat([x[:, ::2, :, :],\n",
        "                        x[:, 1::2, :, :],\n",
        "                       ], -1)\n",
        "        d = torch.matmul(xa, self.ec_d)\n",
        "        s = torch.matmul(xa, self.ec_s)\n",
        "        return d, s\n",
        "       \n",
        "       \n",
        "    def evenOdd(self, x):\n",
        "       \n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        assert ich == 2*self.k\n",
        "        x_e = torch.matmul(x, self.rc_e)\n",
        "        x_o = torch.matmul(x, self.rc_o)\n",
        "       \n",
        "        x = torch.zeros(B, N*2, c, self.k,\n",
        "            device = x.device)\n",
        "        x[..., ::2, :, :] = x_e\n",
        "        x[..., 1::2, :, :] = x_o\n",
        "        return x\n",
        "   \n",
        "   \n",
        "class MWT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ich = 1, k = 3, alpha = 2, c = 1,\n",
        "                 nCZ = 3,\n",
        "                 L = 0,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT,self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.L = L\n",
        "        self.nCZ = nCZ\n",
        "        self.Lk = nn.Linear(ich, c*k)\n",
        "       \n",
        "        self.MWT_CZ = nn.ModuleList(\n",
        "            [MWT_CZ(k, alpha, L, c, base,\n",
        "            initializer) for _ in range(nCZ)]\n",
        "        )\n",
        "        self.Lc0 = nn.Linear(c*k, 128)\n",
        "        self.Lc1 = nn.Linear(128, 2)\n",
        "       \n",
        "        if initializer is not None:\n",
        "            self.reset_parameters(initializer)\n",
        "       \n",
        "    def forward(self, x):\n",
        "       \n",
        "        B, N, ich = x.shape # (B, N, d)\n",
        "        ns = math.floor(np.log2(N))\n",
        "        x = self.Lk(x)\n",
        "        x = x.view(B, N, self.c, self.k)\n",
        "   \n",
        "        for i in range(self.nCZ):\n",
        "            x = torch.tanh(self.MWT_CZ[i](x))\n",
        "            # x = torch.tanh(x)\n",
        "#             x = F.relu(x)\n",
        "#             x = F.leaky_relu(x)\n",
        "\n",
        "        x = x.view(B, N, -1) # collapse c and k\n",
        "        x = self.Lc0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Lc1(x)\n",
        "        return x.squeeze()\n",
        "   \n",
        "    def reset_parameters(self, initializer):\n",
        "        initializer(self.Lc0.weight)\n",
        "        initializer(self.Lc1.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsixvw1PCtCo"
      },
      "outputs": [],
      "source": [
        "# New model\n",
        "ich = 2\n",
        "initializer = get_initializer('xavier_normal') # xavier_normal, kaiming_normal, kaiming_uniform\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "model = MWT(ich,\n",
        "            alpha = 10,\n",
        "            c = 4*4,\n",
        "            k = 4,\n",
        "            base = 'legendre',\n",
        "            nCZ = 2,\n",
        "            initializer = initializer,\n",
        "            )\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 500\n",
        "step_size = 100\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA-eqTr6_Q_d"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "myloss = LpLoss(size_average=False)\n",
        "\n",
        "error_u = []\n",
        "error_v = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_mse = 0\n",
        "    train_l2 = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
        "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
        "        l2.backward() # use the l2 relative loss\n",
        "\n",
        "        optimizer.step()\n",
        "        train_mse += mse.item()\n",
        "        train_l2 += l2.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    model.eval()\n",
        "    test_l2 = 0.0\n",
        "    test_l2_u = 0.0\n",
        "    test_l2_v = 0.0\n",
        "    test_mae = 0.0\n",
        "    test_mae_u = 0.0\n",
        "    test_mae_v = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "            out = model(x)\n",
        "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
        "            test_l2_u += myloss(out[:,:s].view(batch_size, -1), y[:,:s,:].view(batch_size, -1)).item()\n",
        "            test_l2_v += myloss(out[:,s:].view(batch_size, -1), y[:,s:,:].view(batch_size, -1)).item()         \n",
        "\n",
        "    train_l2 /= ntrain\n",
        "    test_l2 /= ntest\n",
        "    test_l2_u /= ntest\n",
        "    test_l2_v /= ntest\n",
        "    error_u.append(test_l2_u)\n",
        "    error_v.append(test_l2_v)\n",
        "\n",
        "    t2 = default_timer()\n",
        "    print(ep, t2-t1, test_l2_u, test_l2_v)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "00ee738ce916cf88a1092a185e07d03424f9fed6277c3e20762079d5b49ec728"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
