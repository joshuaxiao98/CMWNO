{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-M_Nf2gGGa3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpBczXtk_9PV"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Fourier_Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIOLMnnR4QWx"
      },
      "source": [
        "# 1. 1D problem: the (time-independent) Burgers equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tJmIeTa_jw"
      },
      "source": [
        "## 1.1. MWT_1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlIPf1vWcpvk"
      },
      "outputs": [],
      "source": [
        "from utils import train, test, LpLoss, get_filter, UnitGaussianNormalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GcZ4Ec_bU0r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data_utils\n",
        "from typing import List, Tuple\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.io import loadmat, savemat\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import eval_legendre\n",
        "from sympy import Poly, legendre, Symbol\n",
        "import h5py\n",
        "\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu9qtnA-bDJN"
      },
      "outputs": [],
      "source": [
        "# torch.manual_seed(0)\n",
        "# np.random.seed(0)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLc_M6BWbmIU",
        "outputId": "e421756b-b35c-469d-ff6c-00d7ba60609b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 1024)\n",
            "torch.Size([200, 1024])\n",
            "torch.Size([800, 1024, 2])\n",
            "torch.Size([200, 1024, 2])\n",
            "torch.Size([800, 1024, 2])\n",
            "torch.Size([200, 1024])\n"
          ]
        }
      ],
      "source": [
        "# Coupled Data\n",
        "\n",
        "ntrain = 800\n",
        "ntest = 200\n",
        "\n",
        "sub = 2**0 #subsampling rate\n",
        "h = 2**10 // sub #total grid size divided by the subsampling rate\n",
        "s = h\n",
        "\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "rw_u = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Drho_t0_1.mat')\n",
        "x_data = rw_u['rho_t0'].astype(np.float32)\n",
        "y_data = rw_u['rho_t02'].astype(np.float32)\n",
        "print(x_data.shape)\n",
        "\n",
        "x_train_u = x_data[:ntrain,::sub]\n",
        "y_train_u = y_data[:ntrain,::sub]\n",
        "x_test_u = x_data[-ntest:,::sub]\n",
        "y_test_u = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_u = torch.from_numpy(x_train_u)\n",
        "x_test_u = torch.from_numpy(x_test_u)\n",
        "y_train_u = torch.from_numpy(y_train_u)\n",
        "y_test_u = torch.from_numpy(y_test_u)\n",
        "\n",
        "x_train_u = x_train_u.unsqueeze(-1)\n",
        "x_test_u = x_test_u.unsqueeze(-1)\n",
        "\n",
        "\n",
        "rw_v = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Dphi_t0_1.mat')\n",
        "x_data = rw_v['phi_t0'].astype(np.float32)\n",
        "y_data = rw_v['phi_t02'].astype(np.float32)\n",
        "\n",
        "x_train_v = x_data[:ntrain,::sub]\n",
        "y_train_v = y_data[:ntrain,::sub]\n",
        "x_test_v = x_data[-ntest:,::sub]\n",
        "y_test_v = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_v = torch.from_numpy(x_train_v)\n",
        "x_test_v = torch.from_numpy(x_test_v)\n",
        "y_train_v = torch.from_numpy(y_train_v)\n",
        "y_test_v = torch.from_numpy(y_test_v)\n",
        "print(y_test_u.shape)\n",
        "\n",
        "x_train_v = x_train_v.unsqueeze(-1)\n",
        "x_test_v = x_test_v.unsqueeze(-1)\n",
        "\n",
        "x_train = torch.cat([x_train_u.reshape(ntrain,s,1), x_train_v.reshape(ntrain,s,1)], dim=2)\n",
        "x_test = torch.cat([x_test_u.reshape(ntest,s,1), x_test_v.reshape(ntest,s,1)], dim=2)\n",
        "\n",
        "y_train = torch.cat([y_train_u.reshape(ntrain,s,1), y_train_v.reshape(ntrain,s,1)], dim=2)\n",
        "y_test = torch.cat([y_test_u.reshape(ntest,s,1), y_test_v.reshape(ntest,s,1)], dim=2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test[:,:,0].shape)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYBybzLqSUGZ"
      },
      "outputs": [],
      "source": [
        "# with open('data_no_couple/x_train.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br0OphmwSUGa"
      },
      "outputs": [],
      "source": [
        "# with open('data_no_couple/x_test.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(x_test))\n",
        "# with open('data_no_couple/y_train.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(y_train))\n",
        "# with open('data_no_couple/y_test.npy', 'wb') as f:\n",
        "#     np.save(f, np.array(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn7yKvqgcKMA"
      },
      "outputs": [],
      "source": [
        "def get_initializer(name):\n",
        "    \n",
        "    if name == 'xavier_normal':\n",
        "        init_ = partial(nn.init.xavier_normal_)\n",
        "    elif name == 'kaiming_uniform':\n",
        "        init_ = partial(nn.init.kaiming_uniform_)\n",
        "    elif name == 'kaiming_normal':\n",
        "        init_ = partial(nn.init.kaiming_normal_)\n",
        "    return init_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvLWssk3fdKg",
        "outputId": "3d116a72-e4a2-42e5-d86a-a55ae7dc9860"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([800, 1024, 2])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzysmfscb8AB"
      },
      "outputs": [],
      "source": [
        "class sparseKernel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1,\n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernel,self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.Li = nn.Linear(c*k, 128)\n",
        "        self.conv = self.convBlock(c*k, 128)\n",
        "#         self.Lo = nn.Linear(alpha*k, c*k)\n",
        "        self.Lo = nn.Linear(128, c*k)\n",
        "       \n",
        "    def forward(self, x):\n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        x = x.view(B, N, -1)\n",
        "#         x = F.relu(self.Li(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.Lo(x)\n",
        "        x = x.view(B, N, c, ich)\n",
        "       \n",
        "        return x\n",
        "       \n",
        "       \n",
        "    def convBlock(self, ich, och):\n",
        "        net = nn.Sequential(\n",
        "            nn.Conv1d(ich, och, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(och, och, 3, 1, 1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "        )\n",
        "        return net\n",
        "\n",
        "def compl_mul1d(x, weights):\n",
        "    # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "    return torch.einsum(\"bix,iox->box\", x, weights)\n",
        "\n",
        "class sparseKernelFT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1,\n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernelFT, self).__init__()       \n",
        "       \n",
        "        self.modes1 = alpha\n",
        "        self.scale = (1 / (c*k*c*k))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(c*k, c*k, self.modes1, dtype=torch.cfloat))\n",
        "        self.weights1.requires_grad = True\n",
        "        self.k = k\n",
        "       \n",
        "    def forward(self, x):\n",
        "        B, N, c, k = x.shape # (B, N, c, k)\n",
        "       \n",
        "        x = x.view(B, N, -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x_fft = torch.fft.rfft(x)\n",
        "        # Multiply relevant Fourier modes\n",
        "        l = min(self.modes1, N//2+1)\n",
        "        out_ft = torch.zeros(B, c*k, N//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
        "       \n",
        "        out_ft[:, :, :l] = compl_mul1d(x_fft[:, :, :l], self.weights1[:, :, :l])\n",
        "       \n",
        "\n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft(out_ft, n=N)\n",
        "        x = x.permute(0, 2, 1).view(B, N, c, k)\n",
        "        return x\n",
        "       \n",
        "   \n",
        "class MWT_CZ(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k = 3, alpha = 5,\n",
        "                 L = 0, c = 1,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT_CZ, self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.L = L\n",
        "        H0, H1, G0, G1 = get_filter(base, k)\n",
        "       \n",
        "        self.A = sparseKernelFT(k, alpha, c)\n",
        "        self.B = sparseKernelFT(k, alpha, c)\n",
        "        self.C = sparseKernelFT(k, alpha, c)\n",
        "       \n",
        "        self.T0 = nn.Linear(k, k)\n",
        "\n",
        "        self.register_buffer('ec_s', torch.Tensor(\n",
        "            np.concatenate((H0.T, H1.T), axis=0)))\n",
        "        self.register_buffer('ec_d', torch.Tensor(\n",
        "            np.concatenate((G0.T, G1.T), axis=0)))\n",
        "       \n",
        "        self.register_buffer('rc_e', torch.Tensor(\n",
        "            np.concatenate((H0, G0), axis=0)))\n",
        "        self.register_buffer('rc_o', torch.Tensor(\n",
        "            np.concatenate((H1, G1), axis=0)))\n",
        "       \n",
        "       \n",
        "    def forward(self, x, u_d = None, u_s = None):\n",
        "       \n",
        "        B, N, c, ich = x.shape # (B, N, k)\n",
        "        ns = math.floor(np.log2(N))\n",
        "\n",
        "        Ud = torch.jit.annotate(List[Tensor], [])\n",
        "        Us = torch.jit.annotate(List[Tensor], [])\n",
        "#         decompose\n",
        "        for i in range(ns-self.L):\n",
        "            d, x = self.wavelet_transform(x)\n",
        "            Ud += [self.A(d) + self.B(x)]\n",
        "            Us += [self.C(d)]\n",
        "        x = self.T0(x) # coarsest scale transform\n",
        "        #print('x = self.T0(x)',x.shape)\n",
        "\n",
        "        if u_d == None:\n",
        "            for i in range(ns-1-self.L,-1,-1):\n",
        "                x = x + Us[i]\n",
        "                x = torch.cat((x, Ud[i]), -1) # 还没加Ud\n",
        "                x = self.evenOdd(x)\n",
        "                #print('x_none', x.shape)\n",
        "            return x, Ud, Us\n",
        "\n",
        "        else:\n",
        "#        reconstruct \n",
        "\n",
        "            for i in range(ns-1-self.L,-1,-1):\n",
        "                x = x + Us[i] + u_s[i]\n",
        "                x = torch.cat((x, Ud[i]), -1) # 还没加Ud\n",
        "                x = self.evenOdd(x)\n",
        "                #print('x_us', x.shape)\n",
        "            return x, Ud, Us\n",
        "\n",
        "   \n",
        "    def wavelet_transform(self, x):\n",
        "        xa = torch.cat([x[:, ::2, :, :],\n",
        "                        x[:, 1::2, :, :],\n",
        "                       ], -1)\n",
        "        d = torch.matmul(xa, self.ec_d)\n",
        "        s = torch.matmul(xa, self.ec_s)\n",
        "        return d, s\n",
        "       \n",
        "       \n",
        "    def evenOdd(self, x):\n",
        "       \n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        # print(ich)\n",
        "        # print(self.k)\n",
        "        assert ich == 2*self.k\n",
        "        x_e = torch.matmul(x, self.rc_e)\n",
        "        x_o = torch.matmul(x, self.rc_o)\n",
        "       \n",
        "        x = torch.zeros(B, N*2, c, self.k,\n",
        "            device = x.device)\n",
        "        x[..., ::2, :, :] = x_e\n",
        "        x[..., 1::2, :, :] = x_o\n",
        "        return x\n",
        "\n",
        "    def evenOdd_2(self, x):\n",
        "       \n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        # print(ich)\n",
        "        # print(self.k)\n",
        "        assert ich == 2*self.k\n",
        "        x_e = torch.matmul(x, self.rc_e)\n",
        "        x_o = torch.matmul(x, self.rc_o)\n",
        "       \n",
        "        x = torch.zeros(B, N*2, c, self.k,\n",
        "            device = x.device)\n",
        "        x[..., ::2, :, :] = x_e\n",
        "        x[..., 1::2, :, :] = x_o\n",
        "        return x\n",
        "   \n",
        "   \n",
        "class MWT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ich = 1, k = 3, alpha = 2, c = 1,\n",
        "                 nCZ = 3,\n",
        "                 L = 0,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT,self).__init__()\n",
        "       \n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.L = L\n",
        "        self.nCZ = nCZ\n",
        "        self.Lk = nn.Linear(ich, c*k)\n",
        "       \n",
        "        self.MWT_CZ = nn.ModuleList(\n",
        "            [MWT_CZ(k, alpha, L, c, base,\n",
        "            initializer) for _ in range(nCZ)]\n",
        "        )\n",
        "        self.Lc0 = nn.Linear(c*k, 128)\n",
        "        self.Lc1 = nn.Linear(128, 1)\n",
        "       \n",
        "        if initializer is not None:\n",
        "            self.reset_parameters(initializer)\n",
        "       \n",
        "    def forward(self, x, u_d = None, u_s = None):\n",
        "       \n",
        "        B, N, ich = x.shape # (B, N, d)\n",
        "        ns = math.floor(np.log2(N))\n",
        "        x = self.Lk(x)\n",
        "        x = x.view(B, N, self.c, self.k)\n",
        "\n",
        "        Ud = torch.jit.annotate(List[Tensor], [])\n",
        "        Us = torch.jit.annotate(List[Tensor], [])\n",
        "        if u_d == None:\n",
        "            for i in range(self.nCZ):\n",
        "                x, ud, us = self.MWT_CZ[i](x)\n",
        "                x = torch.tanh(x)\n",
        "                Ud += [ud]\n",
        "                Us += [us]\n",
        "            x = x.view(B, N, -1) # collapse c and k\n",
        "            x = self.Lc0(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.Lc1(x)\n",
        "            #print('MWT', x.shape)\n",
        "            return x, Ud, Us\n",
        "          \n",
        "        else:\n",
        "            for i in range(self.nCZ):\n",
        "                x, ud, us = self.MWT_CZ[i](x, u_d[i], u_s[i])\n",
        "                x = torch.tanh(x)\n",
        "                Ud += [ud]\n",
        "                Us += [us]\n",
        "            x = x.view(B, N, -1) # collapse c and k\n",
        "            x = self.Lc0(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.Lc1(x)\n",
        "            return x, Ud, Us\n",
        "   \n",
        "    def reset_parameters(self, initializer):\n",
        "        initializer(self.Lc0.weight)\n",
        "        initializer(self.Lc1.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsixvw1PCtCo"
      },
      "outputs": [],
      "source": [
        "# New model\n",
        "ich = 2\n",
        "initializer = get_initializer('xavier_normal') # xavier_normal, kaiming_normal, kaiming_uniform\n",
        "\n",
        "# torch.manual_seed(0)\n",
        "# np.random.seed(0)\n",
        "model1 = MWT(ich,\n",
        "            alpha = 10,\n",
        "            c = 4*4,\n",
        "            k = 4,\n",
        "            base = 'legendre',\n",
        "            nCZ = 2,\n",
        "            initializer = initializer,\n",
        "            ).to(device)\n",
        "\n",
        "model2 = MWT(ich,\n",
        "            alpha = 10,\n",
        "            c = 4*4,\n",
        "            k = 4,\n",
        "            base = 'legendre',\n",
        "            nCZ = 2,\n",
        "            initializer = initializer,\n",
        "            ).to(device)\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 500\n",
        "step_size = 100\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrI33cWn9rGr"
      },
      "outputs": [],
      "source": [
        "################################################################\n",
        "# training and evaluation\n",
        "################################################################\n",
        "import random\n",
        "\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler1 = torch.optim.lr_scheduler.StepLR(optimizer1, step_size=step_size, gamma=gamma)\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(optimizer2, step_size=step_size, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "GFfH-bry93zi",
        "outputId": "edef8ac5-36de-4b4c-bb6c-fe216fcfed46",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40.0\n",
            "0 0.24808538049459458 0.23404061317443847 0.1660042929649353 0.16820290684700012 19 21\n",
            "1 0.08292121708393096 0.06217746794223786 0.12341262578964234 0.10481873631477356 17 23\n",
            "2 0.05275621771812439 0.04041439786553383 0.07439816832542419 0.07072568535804749 18 22\n",
            "3 0.0510540959239006 0.02572245627641678 0.08290363013744355 0.0663626265525818 15 25\n",
            "4 0.03624243035912514 0.03524769738316536 0.07773786783218384 0.05264899879693985 23 17\n",
            "5 0.04359373554587364 0.02615192160010338 0.05301964998245239 0.05877300262451172 19 21\n",
            "6 0.03323824241757393 0.02187147840857506 0.050553068816661835 0.04257237076759338 19 21\n",
            "7 0.03145403146743774 0.022631964907050134 0.06407972276210785 0.0485115385055542 20 20\n",
            "8 0.03477469876408577 0.020789886116981505 0.057112912833690646 0.039789007008075715 19 21\n",
            "9 0.022489354088902472 0.028375535383820535 0.05333832859992981 0.04473869442939758 25 15\n",
            "10 0.029798105359077454 0.01704562149941921 0.042385170459747316 0.05174964100122452 16 24\n",
            "11 0.021308888271450995 0.02459364488720894 0.050465666353702546 0.044271946251392365 23 17\n",
            "12 0.028515487089753152 0.02170367732644081 0.06259205937385559 0.03228668183088303 23 17\n",
            "13 0.0285090322047472 0.019845146760344504 0.0667203438282013 0.03645507633686065 22 18\n",
            "14 0.024591433480381966 0.019129094183444977 0.047907872200012205 0.03235854506492615 22 18\n",
            "15 0.02250789128243923 0.017927027940750122 0.050137012898921966 0.03489089906215668 22 18\n",
            "16 0.021907244250178336 0.01766651377081871 0.04126094937324524 0.031184268891811372 20 20\n",
            "17 0.025696793720126152 0.017506740018725396 0.042138916850090025 0.032986591756343844 18 22\n",
            "18 0.02477974012494087 0.016534129455685616 0.06072022438049316 0.032865886390209195 20 20\n",
            "19 0.02359129339456558 0.019045491889119148 0.03445481896400451 0.03797854542732239 22 18\n",
            "20 0.01865666061639786 0.019405800998210907 0.038995568752288816 0.033937321305274964 21 19\n",
            "21 0.023842374086380003 0.012466417439281941 0.036384240090847016 0.0311751651763916 16 24\n",
            "22 0.017587045058608055 0.02437381409108639 0.05177061915397644 0.04335584670305252 22 18\n",
            "23 0.02840158127248287 0.018140608742833138 0.03627508759498596 0.03389327257871628 18 22\n",
            "24 0.02007305756211281 0.017197561785578727 0.04197105526924133 0.035148527026176456 19 21\n",
            "25 0.021923376619815825 0.016611735820770263 0.039899027347564696 0.03197361677885056 20 20\n",
            "26 0.019831408485770224 0.016105643659830093 0.03488253057003021 0.03816132485866547 20 20\n",
            "27 0.027089857161045075 0.015299288108944892 0.06119785487651825 0.04807757019996643 17 23\n",
            "28 0.025400476008653642 0.01694964312016964 0.0634141743183136 0.04567750990390777 19 21\n",
            "29 0.014661441445350648 0.024791879504919054 0.06406645238399505 0.029905974864959717 29 11\n",
            "30 0.025180554315447808 0.021219857856631278 0.061219951808452605 0.04360646605491638 22 18\n",
            "31 0.034350070282816884 0.016194652169942855 0.0493168893456459 0.03989123284816742 17 23\n",
            "32 0.023995653688907624 0.019814436361193656 0.0444653058052063 0.042554609179496765 21 19\n",
            "33 0.029389372020959856 0.016134228184819223 0.06064016401767731 0.03747493714094162 17 23\n",
            "34 0.023164357617497446 0.026157714873552323 0.05729376494884491 0.05561479568481445 25 15\n",
            "35 0.029408323392271996 0.023281331583857538 0.04691300541162491 0.057549728751182555 16 24\n",
            "36 0.028340610340237618 0.02132427178323269 0.0813938730955124 0.057718198597431186 21 19\n",
            "37 0.03679159939289093 0.013426117226481437 0.050466359555721284 0.03487912863492966 14 26\n",
            "38 0.02785844437777996 0.01425520695745945 0.04715394347906113 0.03769372552633286 17 23\n",
            "39 0.023018099293112756 0.01864522784948349 0.05697887718677521 0.03305280953645706 21 19\n",
            "40 0.04235544197261334 0.01606786385178566 0.08313506960868836 0.047229070663452145 15 25\n",
            "41 0.019596151635050775 0.03701425276696682 0.06311711847782135 0.0413625892996788 28 12\n",
            "42 0.030470452308654784 0.022981703281402588 0.06489992350339889 0.055956499576568605 20 20\n",
            "43 0.033094733208417895 0.021776695922017097 0.07485101342201234 0.06433404624462127 18 22\n",
            "44 0.03048588678240776 0.021875220015645028 0.058500744700431824 0.05208695381879806 18 22\n",
            "45 0.03142376810312271 0.02070477984845638 0.06956493437290191 0.04056734025478363 19 21\n",
            "46 0.03045446567237377 0.015119081288576126 0.04893013834953308 0.04435149997472763 16 24\n",
            "47 0.016278171315789222 0.02279739361256361 0.04197912096977234 0.036068569719791416 25 15\n",
            "48 0.017586950734257697 0.025239954814314843 0.04831703156232834 0.0450811231136322 24 16\n",
            "49 0.02480282723903656 0.02523362696170807 0.04835231065750122 0.05623022586107254 20 20\n",
            "50 0.02914445623755455 0.022201509550213813 0.049184941947460176 0.05078993946313858 18 22\n",
            "51 0.030749760493636133 0.02093249052762985 0.04466828286647797 0.035764945447444914 20 20\n",
            "52 0.031135246455669403 0.020589280501008035 0.049197591841220856 0.08318815410137176 19 21\n",
            "53 0.02391190007328987 0.025812284722924232 0.05705779045820236 0.03183698385953903 21 19\n",
            "54 0.02159052461385727 0.021850392892956733 0.0667046457529068 0.03423813104629517 24 16\n",
            "55 0.035260034427046776 0.01489488646388054 0.05807580769062042 0.04033177465200424 18 22\n",
            "56 0.037894649133086204 0.017934087365865707 0.06736508429050446 0.03803261458873749 19 21\n",
            "57 0.032585534900426864 0.01471921220421791 0.04704333394765854 0.039043713510036465 16 24\n",
            "58 0.030715383291244507 0.014589287713170052 0.0692113572359085 0.04102333098649979 16 24\n",
            "59 0.020880087465047836 0.024794334694743157 0.053368277847766876 0.04329840779304504 24 16\n",
            "60 0.029717714786529542 0.016964087262749672 0.058101284205913546 0.04022156476974487 17 23\n",
            "61 0.021749840378761293 0.0214670779556036 0.061242689490318296 0.03535507470369339 23 17\n",
            "62 0.024730883166193963 0.020344134196639063 0.04885200709104538 0.03645821243524552 22 18\n",
            "63 0.03077161230146885 0.018800960779190065 0.06053498268127441 0.04276062965393066 20 20\n",
            "64 0.03450104288756847 0.013278939425945283 0.050814613699913025 0.0333110648393631 14 26\n",
            "65 0.02686586797237396 0.01763455495238304 0.05995208829641342 0.034564001262187956 19 21\n",
            "66 0.026914201974868775 0.017248118594288826 0.04253612875938415 0.047816279828548434 16 24\n",
            "67 0.032585953027009965 0.010230489969253541 0.04200365126132965 0.0316355237364769 10 30\n",
            "68 0.022422204539179803 0.01823159836232662 0.04140414357185364 0.04104962706565857 19 21\n",
            "69 0.018721423596143722 0.016684402152895926 0.05143721759319306 0.02785403102636337 23 17\n",
            "70 0.020949302688241005 0.01835502278059721 0.04271101415157318 0.03297345578670502 23 17\n",
            "71 0.019516605213284494 0.019976189509034157 0.04133242070674896 0.04527280747890473 24 16\n",
            "72 0.022695292457938193 0.01512716557830572 0.039447007477283476 0.03410799413919449 19 21\n",
            "73 0.02829696923494339 0.01346238948404789 0.046143530905246734 0.03180590063333511 18 22\n",
            "74 0.02493688724935055 0.012673495635390282 0.049313316345214846 0.03619740635156631 17 23\n",
            "75 0.023450704142451286 0.017935302332043646 0.037523369491100314 0.03339435935020447 18 22\n",
            "76 0.015953176990151405 0.02203667029738426 0.04121673941612244 0.035262128710746764 23 17\n",
            "77 0.030229431986808778 0.012279361262917518 0.05661923974752426 0.03210528075695038 15 25\n",
            "78 0.026341879442334175 0.022033417783677577 0.04594868183135986 0.0664439994096756 18 22\n",
            "79 0.02890119791030884 0.022357467412948608 0.05103910952806473 0.0539448082447052 16 24\n",
            "80 0.019620951861143113 0.020815927237272262 0.050667685866355894 0.02888008624315262 23 17\n",
            "81 0.025158868432044984 0.013090715482831002 0.03665796458721161 0.03669807106256485 18 22\n",
            "82 0.018996048271656036 0.016745580807328225 0.03517374128103256 0.03093431830406189 22 18\n",
            "83 0.020994953960180283 0.01826325595378876 0.03471848577260971 0.0375087159872055 17 23\n",
            "84 0.0208058525621891 0.01753500707447529 0.036515951454639435 0.030272926688194274 20 20\n",
            "85 0.020158580988645553 0.01487817034125328 0.05098550289869309 0.030137570798397063 20 20\n",
            "86 0.0242070010304451 0.014929055273532867 0.042424494922161105 0.031082148551940917 21 19\n",
            "87 0.02801245614886284 0.01148871872574091 0.052495133280754086 0.03159319877624512 15 25\n",
            "88 0.028234754651784898 0.011227041967213153 0.04850584745407104 0.02769504949450493 17 23\n",
            "89 0.030187899172306062 0.007602001763880253 0.03782644748687744 0.03037344813346863 11 29\n",
            "90 0.022387695014476777 0.01147144690155983 0.03511447757482529 0.038760582208633425 15 25\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91 0.01256319709122181 0.01952333275228739 0.03503961086273193 0.027362726628780365 26 14\n",
            "92 0.018468688651919366 0.016090968325734138 0.03821983695030212 0.029526235163211824 22 18\n",
            "93 0.024968395233154295 0.012623917795717716 0.04479918569326401 0.025931684672832488 18 22\n",
            "94 0.022602590397000313 0.01335619680583477 0.051482360363006595 0.02452677384018898 20 20\n",
            "95 0.016623404920101167 0.02160247039049864 0.04670020937919617 0.028991113752126693 26 14\n",
            "96 0.020236612781882286 0.015665228515863418 0.035440749526023864 0.027167800217866897 21 19\n",
            "97 0.024935629777610303 0.010983601249754428 0.04325010240077973 0.032337654232978824 14 26\n",
            "98 0.021276872083544732 0.01327554576098919 0.031454410254955295 0.03787076413631439 16 24\n",
            "99 0.01661281496286392 0.01919791392982006 0.03278954386711121 0.03312756329774857 20 20\n",
            "100 0.01309286780655384 0.010810564681887626 0.02526413157582283 0.01935099646449089 20 20\n",
            "101 0.014282708764076233 0.007315990626811981 0.024547103345394134 0.01939509391784668 16 24\n",
            "102 0.009721075370907783 0.00931857641786337 0.02472523435950279 0.01948846235871315 22 18\n",
            "103 0.01083269391208887 0.007467119432985783 0.021251807510852812 0.018753427267074584 19 21\n",
            "104 0.011286811679601669 0.007649930827319622 0.023436787873506545 0.018374843299388887 18 22\n",
            "105 0.010432395339012145 0.008550699166953564 0.025162653774023058 0.02031163692474365 20 20\n",
            "106 0.010454306565225124 0.00819096878170967 0.023795588612556456 0.018893504142761232 20 20\n",
            "107 0.00889790542423725 0.008684514164924622 0.020977309942245483 0.018139059990644454 22 18\n",
            "108 0.013138542957603931 0.00544952942058444 0.02113219678401947 0.01648336336016655 15 25\n",
            "109 0.010420923978090286 0.007586791422218084 0.020693493485450746 0.017100148499011994 20 20\n",
            "110 0.012450980804860592 0.0074551855772733685 0.02162163570523262 0.019269960969686507 18 22\n",
            "111 0.010356789603829384 0.01036765743046999 0.020018724501132967 0.018046920597553254 21 19\n",
            "112 0.011396082192659378 0.007278643511235714 0.02707381173968315 0.016729127913713455 18 22\n",
            "113 0.010889261960983276 0.007515119891613722 0.020264562219381332 0.024346448332071304 20 20\n",
            "114 0.010936604328453541 0.008968863412737846 0.02333196222782135 0.0204350346326828 18 22\n",
            "115 0.013423085808753968 0.008021834269165994 0.02220052108168602 0.019629458785057067 16 24\n",
            "116 0.010366939567029476 0.01112704187631607 0.025492209196090698 0.020760965198278428 23 17\n",
            "117 0.009315503388643264 0.008850739784538746 0.022122388631105425 0.016429356336593627 23 17\n",
            "118 0.009956016950309277 0.007888686563819646 0.020482623130083085 0.017521986961364745 20 20\n",
            "119 0.010826592743396759 0.006154841054230928 0.022032936662435533 0.01628430813550949 17 23\n",
            "120 0.007543080113828182 0.009011262580752372 0.026146066784858705 0.016389084160327912 25 15\n",
            "121 0.012448490485548974 0.006424183622002601 0.026301500350236894 0.018581422716379165 17 23\n",
            "122 0.01015506785362959 0.00714438596740365 0.024363187402486802 0.016792990267276764 19 21\n",
            "123 0.012007828652858734 0.0069799343869090084 0.020590327978134156 0.025747434198856355 18 22\n",
            "124 0.007788795866072178 0.013111855685710907 0.018834519535303115 0.020226644426584242 23 17\n",
            "125 0.009628192000091075 0.008748950622975826 0.017977388650178908 0.01639189288020134 19 21\n",
            "126 0.010759353376924992 0.00725467212498188 0.02422244518995285 0.01703614801168442 19 21\n",
            "127 0.014783637449145317 0.004919918775558471 0.021868050396442414 0.017085738331079483 13 27\n",
            "128 0.013408464938402176 0.007569484561681748 0.021378527283668517 0.018930925279855727 17 23\n",
            "129 0.010550376772880555 0.007959598544985055 0.02203916445374489 0.015833882540464403 20 20\n",
            "130 0.010697806105017662 0.007420159094035626 0.02400717347860336 0.015551586002111435 21 19\n",
            "131 0.013221942856907844 0.007026607505977154 0.02148275688290596 0.01951039344072342 17 23\n",
            "132 0.010172931030392646 0.007281126584857702 0.0204707869887352 0.016292293518781663 19 21\n",
            "133 0.007217441126704216 0.008596056774258613 0.019268537014722822 0.01535857617855072 25 15\n",
            "134 0.00688511960208416 0.008757825177162885 0.02078413724899292 0.019569664746522903 24 16\n",
            "135 0.008222471252083778 0.00857170544564724 0.024080418050289154 0.022460199743509292 23 17\n",
            "136 0.0097685956209898 0.01047323364764452 0.022446949034929276 0.019488654434680938 21 19\n",
            "137 0.009391852617263795 0.009655010625720025 0.019810567796230315 0.031346912384033206 19 21\n",
            "138 0.01088969398289919 0.007929119318723678 0.022632181644439697 0.015816808938980104 16 24\n",
            "139 0.009113971404731273 0.008937255758792163 0.023705282658338548 0.015369311720132828 24 16\n",
            "140 0.007563156187534332 0.009561101514846087 0.021550193428993225 0.015041755586862564 26 14\n",
            "141 0.010761668607592582 0.008019997887313366 0.01932603567838669 0.024756369143724442 18 22\n",
            "142 0.011055130772292613 0.007454246357083321 0.02050375774502754 0.02024582967162132 16 24\n",
            "143 0.00828294973820448 0.007215304486453533 0.020093283653259277 0.013579253777861596 21 19\n",
            "144 0.007723603509366512 0.00780901875346899 0.018406190425157547 0.014684766083955764 22 18\n",
            "145 0.008795290999114514 0.007981780897825957 0.019199052453041078 0.01752558559179306 22 18\n",
            "146 0.01047223422676325 0.008185574784874916 0.01808657616376877 0.023113011121749877 18 22\n",
            "147 0.007995507419109345 0.010889597591012717 0.021043445765972137 0.018283701986074447 24 16\n",
            "148 0.012244840301573277 0.008180864471942186 0.020867019295692443 0.01629524827003479 19 21\n",
            "149 0.012528054900467395 0.007824762817472219 0.025210856795310974 0.017061023116111754 19 21\n",
            "150 0.008131808377802372 0.009055787026882171 0.023075415194034575 0.0188905930519104 25 15\n",
            "151 0.012520726770162582 0.008024971336126327 0.027976185977458955 0.014968912452459335 20 20\n",
            "152 0.011969595644623042 0.00655077239498496 0.01665812075138092 0.01703458100557327 15 25\n",
            "153 0.011209423393011094 0.007289829384535551 0.030442979335784912 0.015236732810735702 17 23\n",
            "154 0.011002090163528919 0.0070931012555956845 0.030101273357868195 0.014727018773555756 22 18\n",
            "155 0.013046903051435948 0.006484542544931173 0.02288894772529602 0.015626835972070693 19 21\n",
            "156 0.009058538004755973 0.006551167666912079 0.018234997242689132 0.015297002792358398 19 21\n",
            "157 0.010290490053594113 0.0068499698117375375 0.01674098700284958 0.014113304167985917 18 22\n",
            "158 0.009953167587518692 0.008061503488570453 0.018706925213336945 0.016760474145412443 18 22\n",
            "159 0.01036347858607769 0.00646175317466259 0.01947884410619736 0.015359096676111222 14 26\n",
            "160 0.011185631491243839 0.0067394527420401576 0.02120850294828415 0.021491662561893464 18 22\n",
            "161 0.011514544002711773 0.00875708283856511 0.023270528614521026 0.01871525689959526 19 21\n",
            "162 0.011492247097194194 0.007524355873465538 0.02572550803422928 0.019046325236558914 20 20\n",
            "163 0.012015861980617047 0.0065787389874458315 0.016104633137583733 0.017502018362283708 18 22\n",
            "164 0.009128663055598736 0.0074756545014679435 0.017209145426750182 0.016638013273477553 19 21\n",
            "165 0.008693051561713218 0.007011089287698269 0.020025104731321335 0.01664055183529854 19 21\n",
            "166 0.012570029944181443 0.004894745461642742 0.017199117243289947 0.01724205732345581 13 27\n",
            "167 0.011720138341188431 0.005155036002397537 0.01630249887704849 0.02156236082315445 13 27\n",
            "168 0.007155634686350823 0.009254369288682937 0.015891853421926498 0.015215410888195037 22 18\n",
            "169 0.007301800921559334 0.009588556867092847 0.023020359575748443 0.01604006379842758 24 16\n",
            "170 0.012153160087764263 0.007229134775698185 0.02691843345761299 0.019312353730201723 19 21\n",
            "171 0.012500244863331319 0.00731658948585391 0.024858734309673308 0.020931860208511354 19 21\n",
            "172 0.009469944052398205 0.008216654472053051 0.017152122557163238 0.018088803440332413 18 22\n",
            "173 0.013721740879118443 0.004719484318047762 0.02490509510040283 0.014872415810823441 14 26\n",
            "174 0.010028574839234352 0.006878682412207127 0.024632460623979568 0.014587957561016083 20 20\n",
            "175 0.009911481253802777 0.007280856333673 0.025031714141368865 0.013215046972036362 22 18\n",
            "176 0.006998945139348507 0.007837306894361972 0.021890123933553697 0.012734122723340988 26 14\n",
            "177 0.008945072703063488 0.007226624377071857 0.01924319565296173 0.013400535434484481 22 18\n",
            "178 0.009149229861795902 0.006312470361590386 0.017049884647130965 0.014741191864013672 18 22\n",
            "179 0.011060238145291805 0.006323164291679859 0.022234951555728914 0.016520777195692064 16 24\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180 0.010512444116175175 0.006044730581343174 0.022424929589033128 0.015933233499526977 18 22\n",
            "181 0.00813626665621996 0.007346155606210231 0.021067019253969192 0.013826128244400025 22 18\n",
            "182 0.010138191357254982 0.006482642404735089 0.02036666288971901 0.013274574875831604 19 21\n",
            "183 0.009254852458834648 0.006449296213686466 0.016440258026123047 0.014968543201684953 21 19\n",
            "184 0.007750006373971701 0.007179536148905754 0.01838253900408745 0.027664468586444856 21 19\n",
            "185 0.010990630593150854 0.007112345360219478 0.020657148212194443 0.015246567279100419 13 27\n",
            "186 0.010518436469137669 0.00872484989464283 0.021452539563179017 0.01469816341996193 21 19\n",
            "187 0.010454492438584565 0.00749140165746212 0.018525629937648773 0.018773097544908524 18 22\n",
            "188 0.010610563978552819 0.0078027226589620115 0.02467328906059265 0.014110978841781616 20 20\n",
            "189 0.01072739988565445 0.006385000329464674 0.016561760902404785 0.023550601452589037 17 23\n",
            "190 0.009969261288642884 0.008065066374838352 0.01829883798956871 0.018882206231355666 18 22\n",
            "191 0.006403964124619961 0.008499024361371994 0.018804635405540466 0.014692102000117302 24 16\n",
            "192 0.009045351259410382 0.007608146443963051 0.018058581203222276 0.017129755318164824 19 21\n",
            "193 0.01017504345625639 0.0056259710714221004 0.01571179062128067 0.015424578338861466 16 24\n",
            "194 0.008447833359241486 0.007146718334406614 0.019323762208223343 0.01735417515039444 18 22\n",
            "195 0.0067599968425929545 0.0064855988696217535 0.015222295969724655 0.013240274488925934 20 20\n",
            "196 0.007900945264846087 0.004869752451777458 0.025247517824172973 0.012050768062472343 18 22\n",
            "197 0.01095070295035839 0.00551649833098054 0.02172846183180809 0.013676801100373268 18 22\n",
            "198 0.00819693472236395 0.005845042429864406 0.01808047205209732 0.0119567309319973 21 19\n",
            "199 0.008867543619126082 0.004992056898772717 0.016220241338014602 0.017369826287031175 16 24\n",
            "200 0.005937717352062464 0.006510210335254669 0.013104371875524521 0.01196576751768589 21 19\n",
            "201 0.006238361112773419 0.004702311027795076 0.01303522989153862 0.011819267347455024 18 22\n",
            "202 0.004335486870259047 0.005822196118533612 0.014419343695044518 0.010425103455781936 25 15\n",
            "203 0.005509584918618202 0.00436245696619153 0.012982075214385986 0.010998873189091683 20 20\n",
            "204 0.00576278142631054 0.004273045063018799 0.015746971666812895 0.011462726593017579 18 22\n",
            "205 0.006073021255433559 0.004450408592820167 0.012453809753060341 0.009982856884598733 20 20\n",
            "206 0.006444075275212526 0.004201311953365803 0.013521000891923905 0.009846949279308318 20 20\n",
            "207 0.0052293998189270495 0.004919750671833753 0.013456124439835548 0.009750453978776932 23 17\n",
            "208 0.005342591628432274 0.004511995296925306 0.01298346571624279 0.009739960581064223 21 19\n",
            "209 0.003605648297816515 0.005416913442313671 0.013276508003473282 0.010510647669434547 26 14\n",
            "210 0.005869914386421442 0.0036532114818692207 0.013751212283968925 0.009531136751174927 18 22\n",
            "211 0.0054815633781254295 0.0045828684419393535 0.013845677003264428 0.010399211868643761 21 19\n",
            "212 0.005217303838580847 0.004758449196815491 0.014932307824492454 0.010517205819487572 21 19\n",
            "213 0.0067402121983468535 0.0038670446164906025 0.013368212729692458 0.010874605998396873 16 24\n",
            "214 0.004750921931117773 0.004903556834906339 0.013217553794384003 0.011123389899730683 22 18\n",
            "215 0.005421898402273655 0.004354939144104719 0.015512185990810395 0.009464349523186684 20 20\n",
            "216 0.007531516216695309 0.0033989829383790494 0.016125741228461266 0.009862112179398537 16 24\n",
            "217 0.007361763529479504 0.0033998519740998743 0.013700212687253951 0.010171853601932526 16 24\n",
            "218 0.0069551939889788625 0.003438284024596214 0.013140868246555328 0.010125295594334602 16 24\n",
            "219 0.005651636086404324 0.004310771208256483 0.01426268070936203 0.009629247561097145 21 19\n",
            "220 0.007893800623714925 0.0038393049128353594 0.015659319460391997 0.011099312976002694 17 23\n",
            "221 0.0059499265812337394 0.004736499581485987 0.013514000326395034 0.010247162356972695 20 20\n",
            "222 0.007733074724674225 0.003097438998520374 0.014066747426986694 0.010304991602897644 13 27\n",
            "223 0.003992601521313191 0.0053311074338853355 0.013486594930291175 0.009761364385485648 25 15\n",
            "224 0.005982669647783041 0.00367529671639204 0.014424618035554886 0.009712856411933899 18 22\n",
            "225 0.006872145030647516 0.0038970380835235117 0.014293710961937904 0.01072582758963108 18 22\n",
            "226 0.005076627172529697 0.004271834082901478 0.013179979920387268 0.009940446764230728 20 20\n",
            "227 0.0039323937334120275 0.0049636542052030564 0.012931478172540666 0.009459486454725266 24 16\n",
            "228 0.003899147994816303 0.0055065259523689744 0.013472968190908432 0.012004437819123269 25 15\n",
            "229 0.0049198511429131035 0.0060576538369059565 0.014303351864218711 0.01289337031543255 23 17\n",
            "230 0.005515845529735088 0.006100092809647322 0.013159062266349792 0.011870288774371147 24 16\n",
            "231 0.004490584656596183 0.005088064223527909 0.013080725967884064 0.009292038530111313 23 17\n",
            "232 0.006388380005955696 0.0038797524943947792 0.01526530846953392 0.010226420760154724 18 22\n",
            "233 0.0064682537317276 0.005358211118727922 0.014674441143870353 0.014161793887615204 21 19\n",
            "234 0.006064770594239235 0.006353797893971205 0.01668137788772583 0.010457753390073776 23 17\n",
            "235 0.005026030447334051 0.005249976273626089 0.01239940032362938 0.010016495808959008 23 17\n",
            "236 0.004726265836507082 0.0046914535015821456 0.011986278370022774 0.010405301749706268 21 19\n",
            "237 0.005850201547145844 0.00433187298476696 0.013177639320492745 0.010723760202527046 18 22\n",
            "238 0.006026249509304762 0.004182650111615658 0.013673981800675392 0.011372786089777947 18 22\n",
            "239 0.00589171314612031 0.0041820892319083215 0.014679172188043594 0.01472975268959999 17 23\n",
            "240 0.006433941144496203 0.005170189049094915 0.013939696028828621 0.010842520520091057 19 21\n",
            "241 0.004757052958011627 0.00525310032069683 0.01441858522593975 0.010140841975808144 24 16\n",
            "242 0.0061608972027897835 0.003531044702976942 0.012828556448221206 0.009467959180474281 17 23\n",
            "243 0.0048085802420973775 0.004640945736318827 0.013016428127884864 0.009596008732914925 22 18\n",
            "244 0.004477623514831066 0.004988373126834631 0.012611119225621224 0.009844999089837074 23 17\n",
            "245 0.0042837952636182305 0.004649814534932375 0.012747872099280357 0.009444379657506942 23 17\n",
            "246 0.0045827821269631384 0.004554911702871323 0.011952315419912339 0.009877490252256394 22 18\n",
            "247 0.005837770011276007 0.004332280345261097 0.014126316756010055 0.009683115258812904 19 21\n",
            "248 0.005619442779570818 0.004552271030843258 0.016561242043972014 0.009986505061388016 22 18\n",
            "249 0.004801447372883559 0.005050545148551464 0.013221840634942054 0.009489909633994103 24 16\n",
            "250 0.006221422329545021 0.003777403701096773 0.013323031216859817 0.009135412648320199 19 21\n",
            "251 0.005186757743358612 0.0047456839494407176 0.012593554630875588 0.010343857929110528 22 18\n",
            "252 0.005939848776906728 0.004379095695912838 0.0126944849640131 0.012347626462578774 19 21\n",
            "253 0.007238143663853407 0.004357007853686809 0.014251240119338036 0.013651354983448983 16 24\n",
            "254 0.007406045347452163 0.005837279688566923 0.013374699726700783 0.01346041053533554 18 22\n",
            "255 0.005258801840245724 0.004785264097154141 0.013429630771279335 0.010209267064929008 21 19\n",
            "256 0.0069039582647383215 0.0029042671620845796 0.014379588589072227 0.009099704697728157 14 26\n",
            "257 0.008116793781518937 0.003033786043524742 0.013815297111868859 0.009733125865459442 15 25\n",
            "258 0.005444404594600201 0.0043006791360676285 0.012282499521970748 0.008982532247900963 21 19\n",
            "259 0.006767716631293297 0.003398837875574827 0.016882165372371673 0.010111327990889549 17 23\n",
            "260 0.006533805560320616 0.004259214997291565 0.011948528811335563 0.010213375240564346 20 20\n",
            "261 0.005096897594630718 0.004138432070612907 0.011491350084543227 0.010384514033794402 21 19\n",
            "262 0.005533312950283289 0.0034700126573443415 0.011846883669495583 0.010069753006100655 17 23\n",
            "263 0.005387950129806996 0.004019400384277105 0.014022168666124345 0.009187388569116592 20 20\n",
            "264 0.00440769424661994 0.004558686912059784 0.012607854977250098 0.009036677554249764 23 17\n",
            "265 0.004718609247356653 0.0044802695140242575 0.014691729843616486 0.011044543087482452 22 18\n",
            "266 0.007173173800110817 0.003944705501198769 0.016045948565006254 0.010811531469225883 17 23\n",
            "267 0.005877157393842936 0.0043071117252111435 0.012884834483265876 0.009595239982008934 19 21\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "268 0.005723794531077147 0.0036576007679104804 0.015439308658242225 0.010669715330004692 17 23\n",
            "269 0.007674136348068714 0.003624564055353403 0.011922425255179406 0.014053861275315285 14 26\n",
            "270 0.005305014569312334 0.005444891154766083 0.012424163445830345 0.009551924020051956 21 19\n",
            "271 0.0046318391896784305 0.004416774399578572 0.012218613401055336 0.01009672187268734 22 18\n",
            "272 0.004573736116290092 0.004820569250732661 0.013303346708416939 0.010233909413218499 21 19\n",
            "273 0.005214666426181794 0.005694612227380276 0.013442094549536704 0.012899730578064919 21 19\n",
            "274 0.004777145870029927 0.006101033873856068 0.01259964108467102 0.0125922292470932 22 18\n",
            "275 0.004644629657268524 0.005333538986742496 0.013287320882081986 0.010255606397986412 23 17\n",
            "276 0.006805348750203848 0.004303066972643137 0.01275794729590416 0.011143491864204406 18 22\n",
            "277 0.003908324670046568 0.005965116452425718 0.013352604284882545 0.009655711278319358 26 14\n",
            "278 0.006486647482961416 0.003808249440044165 0.012162008434534072 0.009433999583125114 18 22\n",
            "279 0.003448979016393423 0.005226668287068605 0.01254852995276451 0.008610051721334458 27 13\n",
            "280 0.006697590555995702 0.0026473319716751576 0.012493450790643693 0.00927708052098751 14 26\n",
            "281 0.004623247552663088 0.0040756922960281374 0.01265620693564415 0.010075549855828285 21 19\n",
            "282 0.004422537088394165 0.004521934073418379 0.01244025707244873 0.009139454811811448 22 18\n",
            "283 0.00606753109022975 0.003859313875436783 0.012827547490596771 0.011247699782252311 17 23\n",
            "284 0.004539690036326646 0.004609268940985203 0.012359706088900566 0.010974811762571335 22 18\n",
            "285 0.006466101035475731 0.004021017327904702 0.013218794912099839 0.010050219520926475 19 21\n",
            "286 0.005575219728052616 0.004002290479838848 0.01111275963485241 0.010566708445549012 18 22\n",
            "287 0.004791921563446521 0.004065471552312374 0.012481384426355362 0.010308690890669823 19 21\n",
            "288 0.005615306980907917 0.004602510668337345 0.01189529813826084 0.01234010711312294 20 20\n",
            "289 0.007097460404038429 0.003998620714992285 0.01240598276257515 0.010576349571347237 16 24\n",
            "290 0.0043264785967767234 0.004647234734147787 0.01148767299950123 0.00938360795378685 23 17\n",
            "291 0.005565338470041752 0.004254193678498268 0.011443034484982491 0.011749734058976174 19 21\n",
            "292 0.006115626022219658 0.00480200881138444 0.011356470361351966 0.012102356627583504 18 22\n",
            "293 0.0037014477513730524 0.005163469836115837 0.011875496730208396 0.009074494540691376 24 16\n",
            "294 0.005990212392061949 0.0033209893107414247 0.01353239931166172 0.009512024819850922 17 23\n",
            "295 0.005655021257698536 0.004064302910119295 0.012394383549690247 0.009776723757386207 19 21\n",
            "296 0.004224577341228723 0.004952472858130932 0.011422818750143051 0.009279548823833465 23 17\n",
            "297 0.004615394566208124 0.004897001590579748 0.01194252870976925 0.011095739752054214 22 18\n",
            "298 0.004627517629414797 0.0048350456543266775 0.011256034970283508 0.010335573926568031 20 20\n",
            "299 0.0037686646729707717 0.00468343373388052 0.0110204216837883 0.00943304292857647 23 17\n",
            "300 0.004715724810957909 0.002899351604282856 0.011078476160764694 0.00832784853875637 16 24\n",
            "301 0.004664196595549583 0.0029949497804045676 0.010641269385814667 0.008037093877792358 18 22\n",
            "302 0.005328745571896434 0.0020791812054812907 0.010501353964209557 0.008151715993881226 13 27\n",
            "303 0.004473929675295949 0.0025674988888204097 0.010526920855045318 0.008091141954064369 16 24\n",
            "304 0.003698408044874668 0.0032138478010892866 0.011219400465488433 0.0082239580899477 20 20\n",
            "305 0.0034361111745238302 0.0035003109369426966 0.011050283089280128 0.008289881199598313 22 18\n",
            "306 0.004549866504967213 0.002587205395102501 0.010409517288208008 0.007982171773910522 16 24\n",
            "307 0.004055464714765549 0.0031005572061985733 0.009982580989599229 0.007980438917875289 19 21\n",
            "308 0.003579303789883852 0.003324087802320719 0.010302746891975403 0.007916777357459069 21 19\n",
            "309 0.0032332660909742115 0.00347630949690938 0.010109974443912506 0.008044897839426994 22 18\n",
            "310 0.0034333928301930427 0.00332796567119658 0.009867795035243035 0.008059275075793267 21 19\n",
            "311 0.004360368028283119 0.0025946756452322004 0.009953841269016266 0.008352018669247627 16 24\n",
            "312 0.004200334390625358 0.0028476260881870985 0.01029304526746273 0.008233668431639671 18 22\n",
            "313 0.004549240302294493 0.0023055636510252954 0.009899154305458069 0.007984406426548957 15 25\n",
            "314 0.0038370151445269586 0.0030859217699617148 0.010426533818244934 0.007824443131685257 20 20\n",
            "315 0.003871953161433339 0.0029092121962457896 0.010196681320667266 0.008088969960808753 19 21\n",
            "316 0.004080352392047643 0.0027968185395002365 0.010860725194215775 0.007965319752693177 18 22\n",
            "317 0.003644872950389981 0.0032722369115799664 0.010082844719290733 0.007818883657455445 21 19\n",
            "318 0.003618709584698081 0.0030695427116006614 0.010004305243492127 0.007779644727706909 20 20\n",
            "319 0.0034251902624964714 0.003338898690417409 0.009896219670772553 0.007848654016852379 22 18\n",
            "320 0.004465476656332612 0.0023293656297028063 0.010087181851267814 0.00788848489522934 15 25\n",
            "321 0.003430708656087518 0.0030853170715272426 0.009847944676876068 0.007867757752537728 20 20\n",
            "322 0.0034234360232949256 0.003231597049161792 0.011057024672627449 0.0078997553139925 21 19\n",
            "323 0.00415462370030582 0.0026645486522465945 0.009735930114984512 0.007782052084803581 17 23\n",
            "324 0.0037415874470025303 0.002924028765410185 0.00989811785519123 0.00780382938683033 19 21\n",
            "325 0.003506702147424221 0.0032289839070290326 0.01062305897474289 0.008572150841355325 21 19\n",
            "326 0.004055856820195913 0.0029809865448623897 0.009785824120044709 0.008131743371486665 18 22\n",
            "327 0.00410080723464489 0.0031426716502755882 0.009606692641973495 0.008187485337257385 19 21\n",
            "328 0.0038286463730037214 0.0030566709954291584 0.0095547404140234 0.007931883856654167 19 21\n",
            "329 0.0037927351519465445 0.0025757422111928464 0.01006038598716259 0.007828463092446327 17 23\n",
            "330 0.0030821653082966803 0.003752841455861926 0.011360285133123397 0.007852452695369721 24 16\n",
            "331 0.0044058611243963245 0.0028711905237287284 0.011150756552815437 0.008325178250670433 18 22\n",
            "332 0.0031233198288828133 0.003937501823529601 0.010862979367375373 0.007982320189476012 24 16\n",
            "333 0.004358519874513149 0.002843368388712406 0.00982655256986618 0.008421860113739967 17 23\n",
            "334 0.0038866613060235975 0.0031684121862053873 0.009789785221219063 0.008480601683259011 18 22\n",
            "335 0.0030844350904226303 0.003614879148080945 0.010272445380687714 0.007919769287109375 22 18\n",
            "336 0.0037462329864501953 0.003291385127231479 0.010020337179303168 0.008032954707741737 21 19\n",
            "337 0.0038997914828360083 0.003561930963769555 0.00995763048529625 0.007933383360505104 22 18\n",
            "338 0.0029648822732269765 0.0036396031081676484 0.009819707572460175 0.008082725629210473 23 17\n",
            "339 0.004613363640382886 0.002564167119562626 0.010291007384657859 0.007866471111774444 16 24\n",
            "340 0.003154280008748174 0.0036912060528993605 0.009799463525414467 0.007748081907629966 23 17\n",
            "341 0.0036910164542496206 0.003486051242798567 0.010390853062272071 0.008555590882897377 22 18\n",
            "342 0.00459245104342699 0.002496153013780713 0.009671730920672417 0.008089298233389854 16 24\n",
            "343 0.0038667346350848677 0.003072303170338273 0.010811166763305664 0.008113955929875374 19 21\n",
            "344 0.003992875199764967 0.0031806911062449215 0.009356277585029603 0.007638370469212532 20 20\n",
            "345 0.0042031117342412475 0.0029694488737732174 0.010370487198233604 0.008182665333151817 19 21\n",
            "346 0.005400599259883165 0.0019506258796900512 0.011177724599838257 0.008310126289725304 12 28\n",
            "347 0.004333329554647207 0.0029982797615230082 0.00979246847331524 0.00801278218626976 17 23\n",
            "348 0.004861415950581431 0.002275363327935338 0.010647290870547294 0.008420135155320167 14 26\n",
            "349 0.004276212323457002 0.0027237186394631864 0.010130878090858459 0.007652637735009194 17 23\n",
            "350 0.004199841236695648 0.002676397692412138 0.009914577007293701 0.007725941613316536 17 23\n",
            "351 0.003952118121087551 0.003080618977546692 0.010314365476369857 0.007651079371571541 20 20\n",
            "352 0.0036924089305102824 0.0032263771258294583 0.010060907378792762 0.007902585044503212 21 19\n",
            "353 0.0029464556463062765 0.004010539231821895 0.009427730590105057 0.008266783952713012 24 16\n",
            "354 0.0042651833407580855 0.003097571860998869 0.01006212629377842 0.00835304282605648 18 22\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "355 0.004943603752180934 0.002472388157621026 0.010290777012705803 0.008609583228826522 15 25\n",
            "356 0.002796760108321905 0.004558908650651574 0.01099583700299263 0.00818860024213791 26 14\n",
            "357 0.00416551947593689 0.0032435795199126007 0.012033376842737198 0.007696583792567253 20 20\n",
            "358 0.003034762805327773 0.0040735981054604055 0.011256203949451447 0.008237756565213203 25 15\n",
            "359 0.0031557362154126167 0.003890512641519308 0.009770919755101203 0.007733739241957665 24 16\n",
            "360 0.0028630274999886752 0.004355333978310227 0.010067617893218994 0.00850762464106083 25 15\n",
            "361 0.003828423162922263 0.0034197603072971105 0.009337407499551774 0.007697374895215035 21 19\n",
            "362 0.0043930233642458915 0.0024271564185619353 0.009892198964953422 0.007813887968659401 16 24\n",
            "363 0.003609725907444954 0.0029933554958552124 0.010632247254252434 0.007595236226916313 20 20\n",
            "364 0.003853918667882681 0.002949226489290595 0.009461692720651626 0.007902869433164596 19 21\n",
            "365 0.004390624202787876 0.0023080836888402703 0.009368822723627091 0.008550318852066995 14 26\n",
            "366 0.0034619346633553505 0.003405245766043663 0.009136266857385635 0.008270294591784477 20 20\n",
            "367 0.003471696823835373 0.0029128606803715228 0.009604513570666313 0.007758613526821136 19 21\n",
            "368 0.004091365793719888 0.0030061722826212646 0.010250448063015938 0.007860061377286912 20 20\n",
            "369 0.00355003796517849 0.003171015456318855 0.01053959757089615 0.008067133203148842 21 19\n",
            "370 0.0033189352601766587 0.003197197308763862 0.009646382704377175 0.007603289410471916 21 19\n",
            "371 0.004481885973364115 0.0029781741369515656 0.011506604552268982 0.008262641206383705 19 21\n",
            "372 0.004335475061088801 0.003131998786702752 0.010210828930139542 0.00868737444281578 19 21\n",
            "373 0.0032782216649502515 0.004134287601336837 0.01032366655766964 0.008026300817728043 24 16\n",
            "374 0.003873719647526741 0.003465711437165737 0.010322935432195663 0.007656631916761398 21 19\n",
            "375 0.0041479097865521905 0.0029003031365573404 0.010338536351919175 0.007960979640483857 19 21\n",
            "376 0.004624143550172448 0.0025098600331693887 0.009109469056129455 0.007571945115923882 16 24\n",
            "377 0.0032627425994724035 0.0032017774134874342 0.0102634996175766 0.0075607261806726455 21 19\n",
            "378 0.003486171206459403 0.0029253722075372936 0.00934504009783268 0.007683132141828537 19 21\n",
            "379 0.003846671124920249 0.002980788629502058 0.00972504511475563 0.008619240149855614 19 21\n",
            "380 0.003526661042124033 0.0027739774249494076 0.00904329776763916 0.0074560770392417906 18 22\n",
            "381 0.00355040667578578 0.00296620630659163 0.01007008083164692 0.007935302555561066 19 21\n",
            "382 0.004156208420172333 0.0026413442101329565 0.009408429712057114 0.007627832517027855 17 23\n",
            "383 0.0038218457903712987 0.002852289481088519 0.009258536472916603 0.007825782671570778 19 21\n",
            "384 0.0031298288237303494 0.0032027236744761466 0.009182564914226532 0.008175225034356116 21 19\n",
            "385 0.0030447425693273544 0.003259298866614699 0.009048480167984962 0.007672434002161026 21 19\n",
            "386 0.0038843788672238587 0.002790681030601263 0.009817281812429428 0.007640692889690399 18 22\n",
            "387 0.003944593109190464 0.0025303983967751263 0.010246680751442909 0.00794926293194294 17 23\n",
            "388 0.004441883843392134 0.0032016580272465943 0.009724297747015952 0.00822517365217209 20 20\n",
            "389 0.003448858056217432 0.0035396136064082383 0.010228005424141883 0.008193495646119119 22 18\n",
            "390 0.004223432606086135 0.002680464740842581 0.009719565138220787 0.007714899554848671 17 23\n",
            "391 0.003455431703478098 0.003461814941838384 0.010852613374590874 0.007764846608042717 22 18\n",
            "392 0.004874330721795559 0.0026070948876440523 0.009320407658815383 0.007584213986992836 16 24\n",
            "393 0.004042858043685555 0.0025349177047610284 0.01003734365105629 0.007888421788811684 16 24\n",
            "394 0.0030526147410273553 0.0036101899482309818 0.008937032744288444 0.0075890436023473735 24 16\n",
            "395 0.004271235782653093 0.002240214478224516 0.009049765169620513 0.0075597652792930604 15 25\n",
            "396 0.00260783139616251 0.003398276176303625 0.009026079848408699 0.0073055771738290785 23 17\n",
            "397 0.0026230340730398895 0.003460435215383768 0.009013676419854164 0.007342494577169418 24 16\n",
            "398 0.00327469427138567 0.0029246603790670633 0.00940882258117199 0.007552709579467774 20 20\n",
            "399 0.0033924478106200696 0.002796226106584072 0.008975285440683364 0.007656167969107628 19 21\n",
            "400 0.002383116018027067 0.0036475185677409174 0.009026652723550797 0.007195664420723915 25 15\n",
            "401 0.0032368089444935323 0.002447865689173341 0.00870344541966915 0.007067046090960503 18 22\n",
            "402 0.002531670443713665 0.002945967027917504 0.008743481189012527 0.0071119174361228945 22 18\n",
            "403 0.002546429932117462 0.0029631507396697997 0.008685178831219673 0.0070692914724349975 22 18\n",
            "404 0.002625047853216529 0.0028206915874034164 0.008498479500412942 0.007143903002142907 21 19\n",
            "405 0.002481121867895126 0.0029914317559450866 0.008569575771689415 0.007025197520852089 22 18\n",
            "406 0.0030259016435593366 0.002581828748807311 0.008911748677492141 0.007190055400133133 19 21\n",
            "407 0.0030236377008259294 0.0025496473442763092 0.008568922057747841 0.007012722641229629 19 21\n",
            "408 0.0029896857403218747 0.002448464650660753 0.008499602600932122 0.007098545879125595 19 21\n",
            "409 0.002492637308314443 0.0029255055822432043 0.008816717639565468 0.007072256505489349 22 18\n",
            "410 0.002064002463594079 0.00334953248500824 0.008539358004927635 0.007083647251129151 25 15\n",
            "411 0.0036387684103101493 0.00185996288433671 0.008520359620451928 0.00713048130273819 14 26\n",
            "412 0.003108749222010374 0.0025992256309837103 0.008617118299007416 0.007084433063864708 19 21\n",
            "413 0.0032953489758074284 0.0022371774259954693 0.00871163710951805 0.007121230363845825 17 23\n",
            "414 0.004001826867461204 0.001488770069554448 0.008470296040177345 0.007213585004210472 11 29\n",
            "415 0.0026511392649263142 0.0028228538762778044 0.00846685364842415 0.007211674079298973 21 19\n",
            "416 0.0030852521676570177 0.002598638515919447 0.008416380286216735 0.007065514922142029 19 21\n",
            "417 0.00412878136150539 0.0016105743404477835 0.00881633661687374 0.007022714912891388 12 28\n",
            "418 0.0026417539268732072 0.002945920219644904 0.008636697307229041 0.007007526457309723 22 18\n",
            "419 0.0025000231806188824 0.0029577014688402414 0.008367484509944916 0.007063180357217789 22 18\n",
            "420 0.0022714438196271658 0.003111223392188549 0.008352223634719848 0.0070436645299196245 23 17\n",
            "421 0.0026227054838091133 0.0027790559455752374 0.00836079590022564 0.0071589011698961254 21 19\n",
            "422 0.003712931163609028 0.0020004162937402726 0.009882582873106003 0.007088255733251571 15 25\n",
            "423 0.0034047276340425014 0.002182763423770666 0.008591215014457703 0.007042929977178574 17 23\n",
            "424 0.0023811381217092274 0.0032697349414229394 0.008472185581922531 0.0072352422028779985 24 16\n",
            "425 0.002414248129352927 0.003069221926853061 0.008489315882325172 0.0070417220890522 23 17\n",
            "426 0.0024995515681803227 0.0030047064926475285 0.008543824702501297 0.0070718488097190856 22 18\n",
            "427 0.0026739201694726943 0.00281455266289413 0.008333102986216544 0.00703765220940113 21 19\n",
            "428 0.003025523405522108 0.002386497827246785 0.00844502165913582 0.0069762785732746125 18 22\n",
            "429 0.002573844101279974 0.0028060618229210376 0.008342463970184327 0.0071269901096820834 21 19\n",
            "430 0.003149762228131294 0.002370888637378812 0.008457833379507064 0.007116434425115586 18 22\n",
            "431 0.003383674547076225 0.0021832642797380685 0.008368445932865143 0.007102805376052857 16 24\n",
            "432 0.002950315224006772 0.002534873727709055 0.008561372086405753 0.006953129395842552 19 21\n",
            "433 0.0025087400060147045 0.0030112125165760516 0.008539397493004799 0.006997385546565055 22 18\n",
            "434 0.002191017447039485 0.0031929855700582265 0.008225244134664536 0.006970844119787216 24 16\n",
            "435 0.0027987550105899573 0.0025519001483917236 0.008464093878865242 0.007200483605265618 19 21\n",
            "436 0.0035635753441601993 0.0021328413859009743 0.00834684781730175 0.007324739098548889 15 25\n",
            "437 0.0038164100982248784 0.0018522903323173524 0.008487325385212898 0.007477342113852501 13 27\n",
            "438 0.0032814555056393147 0.0024422175530344247 0.008314782530069351 0.007230425551533699 17 23\n",
            "439 0.0037501655891537668 0.001818489721044898 0.008523400649428367 0.00723489373922348 13 27\n",
            "440 0.0021883105393499134 0.0034109739772975444 0.008399845138192176 0.0071748320758342745 25 15\n",
            "441 0.002514834674075246 0.0029391540586948396 0.00843471884727478 0.0071620200574398045 22 18\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "442 0.002574257776141167 0.002940968982875347 0.008402015268802642 0.006980651989579201 22 18\n",
            "443 0.0034934216178953647 0.001994940135627985 0.008257781341671943 0.007173057198524475 15 25\n",
            "444 0.0029894096218049527 0.0024457524344325067 0.008449212312698365 0.007004214376211166 18 22\n",
            "445 0.002813160195946693 0.0027103321719914673 0.008365206345915794 0.006970553547143936 20 20\n",
            "446 0.002376237753778696 0.002920274576172233 0.00821463979780674 0.0069438529014587405 22 18\n",
            "447 0.0024541858863085506 0.0030003426410257815 0.008624996095895766 0.007013280466198921 22 18\n",
            "448 0.002234261818230152 0.0032428494188934563 0.008454226180911064 0.007015150636434555 24 16\n",
            "449 0.003121725069358945 0.0024048429634422066 0.00865705244243145 0.006985228210687637 18 22\n",
            "450 0.002786744125187397 0.0026438786555081605 0.009201166406273841 0.007041897475719452 20 20\n",
            "451 0.003217737441882491 0.002427955847233534 0.008654546663165093 0.006975045129656791 18 22\n",
            "452 0.0025526187382638454 0.003061288492754102 0.008371107205748558 0.006947650164365768 23 17\n",
            "453 0.0029741794895380737 0.00238291478715837 0.008272392600774765 0.006932984814047813 18 22\n",
            "454 0.0027347506675869226 0.00267477941699326 0.0083043172955513 0.006909698247909546 20 20\n",
            "455 0.0030658534541726113 0.0022765446081757547 0.008295085430145264 0.006889085397124291 17 23\n",
            "456 0.0032534680049866437 0.00229869294911623 0.008205124363303184 0.0070007602125406266 17 23\n",
            "457 0.002882424145936966 0.0026309906039386987 0.008298666551709175 0.006916241534054279 20 20\n",
            "458 0.0013608244992792606 0.004033620217815042 0.00855120837688446 0.0071716724336147305 30 10\n",
            "459 0.003231942970305681 0.0023881212621927263 0.008688566237688065 0.006893191151320934 18 22\n",
            "460 0.002891682703047991 0.0029657428618520497 0.008676358237862588 0.007556067705154419 21 19\n",
            "461 0.0031891552824527027 0.0024618123192340136 0.00810214702039957 0.0073514065146446225 17 23\n",
            "462 0.0028111086785793304 0.002707564663141966 0.008421173617243767 0.007033686265349388 20 20\n",
            "463 0.0025022261682897807 0.0029077520221471786 0.008403244316577912 0.007247469276189804 22 18\n",
            "464 0.002252794299274683 0.003251467701047659 0.008418399393558502 0.007062031701207161 24 16\n",
            "465 0.002855609552934766 0.0025450167804956436 0.008106726258993148 0.006987642794847489 19 21\n",
            "466 0.0030557807721197606 0.002161045577377081 0.008098781853914262 0.0069128919392824175 17 23\n",
            "467 0.002726942552253604 0.0024944198690354824 0.00826442040503025 0.007114191129803657 19 21\n",
            "468 0.002627987042069435 0.002831875756382942 0.008291701525449753 0.007020421326160431 21 19\n",
            "469 0.002384236203506589 0.003086215704679489 0.008303834348917008 0.006969613656401634 23 17\n",
            "470 0.0026067839562892913 0.0028678909968584774 0.008285852447152138 0.007295146435499191 21 19\n",
            "471 0.0020018846355378626 0.003474425720050931 0.00827417552471161 0.007272341623902321 25 15\n",
            "472 0.003772359862923622 0.001855355342850089 0.008242955729365349 0.006955516934394837 14 26\n",
            "473 0.0034019605442881583 0.0021020582038909197 0.008591017201542855 0.006960416436195374 16 24\n",
            "474 0.0033061361126601697 0.0022227453161031007 0.008979873731732368 0.006950200125575065 17 23\n",
            "475 0.0028242429345846174 0.002946405941620469 0.00891510844230652 0.007116449102759361 22 18\n",
            "476 0.0022048167604953053 0.003326416751369834 0.008194295465946197 0.007061929106712341 25 15\n",
            "477 0.0031125079188495876 0.002252182438969612 0.008157032355666161 0.006942463591694831 17 23\n",
            "478 0.0026969963032752275 0.0027321249432861803 0.007984139621257783 0.006898616403341293 21 19\n",
            "479 0.0030166006181389093 0.0023694664798676966 0.00848877191543579 0.006853903010487557 18 22\n",
            "480 0.002841454781591892 0.0027564997784793378 0.008267467990517616 0.006879288032650948 21 19\n",
            "481 0.002829814273864031 0.0026235959585756063 0.008046479970216751 0.006950165070593357 20 20\n",
            "482 0.003492315076291561 0.0017948331870138645 0.008069593608379364 0.006852264478802681 14 26\n",
            "483 0.0026991446409374478 0.002790280133485794 0.008186414018273353 0.006922725960612297 21 19\n",
            "484 0.0033365571033209564 0.0020175152085721492 0.008232521489262581 0.007050598859786987 15 25\n",
            "485 0.0030754642747342588 0.0023843073658645153 0.0081301811337471 0.007028343006968498 18 22\n",
            "486 0.0018566569592803716 0.0035123290307819844 0.008241164460778237 0.007166845127940178 26 14\n",
            "487 0.00309674983844161 0.0022771199606359004 0.008222439661622047 0.006823323257267475 17 23\n",
            "488 0.0022921782452613114 0.002907468993216753 0.008084483593702316 0.006778744831681251 23 17\n",
            "489 0.0023736792523413896 0.0028483294136822223 0.007902869395911694 0.006958880238234997 22 18\n",
            "490 0.0017810876574367285 0.0034853567834943534 0.00794935315847397 0.006805137358605861 27 13\n",
            "491 0.003260702108964324 0.002168448958545923 0.008294443041086197 0.006855850517749787 17 23\n",
            "492 0.0024916230980306865 0.0028155858442187307 0.008196727707982063 0.0067654033750295635 22 18\n",
            "493 0.0025935866869986057 0.0026774276793003083 0.0082738246768713 0.006817658580839634 21 19\n",
            "494 0.0026547127775847913 0.002743895435705781 0.008277500867843629 0.0068674907833337785 21 19\n",
            "495 0.0026183719374239444 0.0026064622960984708 0.007903002575039864 0.006955240592360496 20 20\n",
            "496 0.002651875838637352 0.00284523731097579 0.008440304175019264 0.006778958924114704 21 19\n",
            "497 0.0037103900499641895 0.0019693454541265965 0.008159409761428833 0.006932265236973763 15 25\n",
            "498 0.0030661645997315645 0.002307866867631674 0.00927347905933857 0.006897137500345707 18 22\n",
            "499 0.002523085316643119 0.0031167176738381387 0.00825132742524147 0.007064826339483261 23 17\n"
          ]
        }
      ],
      "source": [
        "# Train and test\n",
        "error_u = []\n",
        "error_v = []\n",
        "mae_error_u = []\n",
        "mae_error_v = []\n",
        "\n",
        "myloss = LpLoss(size_average=False)\n",
        "mse = nn.MSELoss()\n",
        "mae = nn.L1Loss()\n",
        "len_loader = ntrain/batch_size\n",
        "print(len_loader)\n",
        "\n",
        "\n",
        "for ep in range(epochs):\n",
        "    model1.train()\n",
        "    model2.train()\n",
        "    t1 = default_timer()\n",
        "\n",
        "    train_l2_u = 0\n",
        "    train_l2_v = 0\n",
        "\n",
        "    flag_1 = 0\n",
        "    flag_2 = 0\n",
        "    '''\n",
        "    for x, y in train_loader:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "\n",
        "      random_num = random.random()\n",
        "      # NO1->NO2\n",
        "      if random_num < 0.5:\n",
        "\n",
        "        # if flag_1 > len_loader/2:\n",
        "        #   continue\n",
        "\n",
        "        # else:\n",
        "          u_train_mid = torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          #print(u_train_mid.shape)\n",
        "          with torch.no_grad(): # No grad upgrade\n",
        "\n",
        "            #u_out_mid = model1(u_train_mid.detach())\n",
        "            u_out_mid, Ud, Us = model1(x)\n",
        "\n",
        "          #v_train_end = torch.cat([x,  u_out_mid], dim=2)\n",
        "            \n",
        "\n",
        "          optimizer2.zero_grad()\n",
        "\n",
        "          #v_train_end = torch.cat([x], dim=2)\n",
        "\n",
        "          v_out_end, Ud, Us = model2(x, Ud, Us)\n",
        "\n",
        "          l2_v = myloss(v_out_end.view(batch_size, -1), y[:,:,1].view(batch_size, -1))\n",
        "          l2_v.backward() # use the l2 relative loss\n",
        "\n",
        "          optimizer2.step()\n",
        "          train_l2_v += l2_v.item()\n",
        "\n",
        "          flag_1 += 1\n",
        "      \n",
        "      # NO2->NO1\n",
        "      else: \n",
        "        # if flag_2 > len_loader/2:\n",
        "        #   continue\n",
        "        \n",
        "        # else:\n",
        "\n",
        "          v_train_mid =  torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          with torch.no_grad():\n",
        "            v_out_mid, Ud, Us = model2(x)\n",
        "            # print(len(Ud))\n",
        "            # print(Ud[0])\n",
        "          # print(x.shape)\n",
        "          # print(u_out_mid.shape)\n",
        "          #v_train_end = torch.cat([x,  u_out_mid], dim=2)\n",
        "          \n",
        "          \n",
        "          optimizer1.zero_grad()\n",
        "\n",
        "          #u_train_end = torch.cat([x,  v_out_mid], dim=2)\n",
        "          u_out_end, Ud, Us = model1(x, Ud, Us)\n",
        "          l2_u = myloss(u_out_end.view(batch_size, -1), y[:,:,0].view(batch_size, -1))\n",
        "          l2_u.backward() # use the l2 relative loss\n",
        "\n",
        "          optimizer1.step()\n",
        "          train_l2_u += l2_u.item()\n",
        "\n",
        "          flag_2 += 1\n",
        "      '''\n",
        "\n",
        "      for x, y in train_loader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        random_num = random.random()\n",
        "        # NO1->NO2\n",
        "        if random_num < 0.5:\n",
        "\n",
        "          u_train_mid = torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          with torch.no_grad(): # No grad upgrade\n",
        "              u_out_mid, Ud, Us = model1(x)\n",
        "          optimizer2.zero_grad()\n",
        "          v_out_end, Ud, Us = model2(x, Ud, Us)\n",
        "          l2_v = myloss(v_out_end.view(batch_size, -1), y[:,:,1].view(batch_size, -1))\n",
        "          l2_v.backward() # use the l2 relative loss\n",
        "          optimizer2.step()\n",
        "          train_l2_v += l2_v.item()\n",
        "\n",
        "          v_train_mid =  torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          with torch.no_grad():\n",
        "              v_out_mid, Ud, Us = model2(x)\n",
        "          optimizer1.zero_grad()\n",
        "          u_out_end, Ud, Us = model1(x, Ud, Us)\n",
        "          l2_u = myloss(u_out_end.view(batch_size, -1), y[:,:,0].view(batch_size, -1))\n",
        "          l2_u.backward() # use the l2 relative loss\n",
        "          optimizer1.step()\n",
        "          train_l2_u += l2_u.item()\n",
        "\n",
        "        # NO2->NO1\n",
        "        else: \n",
        "\n",
        "          v_train_mid =  torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          with torch.no_grad():\n",
        "              v_out_mid, Ud, Us = model2(x)\n",
        "          optimizer1.zero_grad()\n",
        "          u_out_end, Ud, Us = model1(x, Ud, Us)\n",
        "          l2_u = myloss(u_out_end.view(batch_size, -1), y[:,:,0].view(batch_size, -1))\n",
        "          l2_u.backward() # use the l2 relative loss\n",
        "          optimizer1.step()\n",
        "          train_l2_u += l2_u.item()\n",
        "\n",
        "          u_train_mid = torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "          with torch.no_grad(): # No grad upgrade\n",
        "              u_out_mid, Ud, Us = model1(x)\n",
        "          optimizer2.zero_grad()\n",
        "          v_out_end, Ud, Us = model2(x, Ud, Us)\n",
        "          l2_v = myloss(v_out_end.view(batch_size, -1), y[:,:,1].view(batch_size, -1))\n",
        "          l2_v.backward() # use the l2 relative loss\n",
        "          optimizer2.step()\n",
        "          train_l2_v += l2_v.item()\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "# # Test:\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "    test_l2_u = 0.0\n",
        "    i = 0\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "        #v_test_mid = torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "        v_test_out_mid, Ud, Us = model2(x)\n",
        "        #u_test_end = torch.cat([x,  v_test_out_mid], dim=2)\n",
        "        u_test_out_end, Ud, Us = model1(x,Ud, Us)\n",
        "        u_test_out_end = u_test_out_end.view(batch_size, -1)\n",
        "        y = y[:,:,0].view(batch_size, -1)\n",
        "        if i == 0:\n",
        "            u_pred =  u_test_out_end\n",
        "            u_label = y\n",
        "            i += 1\n",
        "        else:\n",
        "            u_pred = torch.cat((u_pred, u_test_out_end),0)\n",
        "            u_label = torch.cat((u_label, y), 0)\n",
        "        \n",
        "        test_l2_u += myloss(u_test_out_end, y).item()\n",
        "\n",
        "    train_l2_u /= ntrain\n",
        "    test_l2_u /= ntest\n",
        "    error_u.append(test_l2_u)\n",
        "#     mse_u = mse(u_pred, u_label)\n",
        "#     rmse_u = torch.sqrt(mse_u)\n",
        "    mae_u = mae(u_pred, u_label)\n",
        "\n",
        "    test_l2_v = 0.0\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "            #u_test_mid = torch.cat([x,  torch.zeros([batch_size,s,1]).cuda()], dim=2)\n",
        "            u_test_out_mid, Ud, Us = model1(x)\n",
        "            #v_test_end = torch.cat([x,  u_test_out_mid], dim=2)\n",
        "            v_test_out_end, Ud, Us = model2(x, Ud, Us)\n",
        "            \n",
        "            v_test_out_end = v_test_out_end.view(batch_size, -1)\n",
        "            y = y[:,:,1].view(batch_size, -1)\n",
        "            if i == 0:\n",
        "                v_pred =  v_test_out_end\n",
        "                v_label = y\n",
        "                i += 1\n",
        "            else:\n",
        "                v_pred = torch.cat((v_pred, v_test_out_end),0)\n",
        "                v_label = torch.cat((v_label, y), 0)\n",
        "            test_l2_v += myloss(v_test_out_end, y).item()\n",
        "\n",
        "    train_l2_v /= ntrain\n",
        "    test_l2_v /= ntest\n",
        "    error_v.append(test_l2_v)\n",
        "#     mse_v = mse(v_pred, v_label)\n",
        "#     rmse_v = torch.sqrt(mse_v)\n",
        "    mae_v = mae(v_pred, v_label)\n",
        "    mae_error_u.append(mae_u.item())\n",
        "    mae_error_v.append(mae_v.item())\n",
        "\n",
        "\n",
        "    print(ep,train_l2_u, train_l2_v,test_l2_u, test_l2_v,mae_u.item(),mae_v.item())\n",
        "\n",
        "np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_u_l0l0_ugrf_vgrf_SOTA_error',error_u)\n",
        "np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_v_l0l0_ugrf_vgrf_SOTA_error',error_v)\n",
        "np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_u_l0l0_ugrf_vgrf_SOTA_mae',mae_error_u)\n",
        "np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_v_l0l0_ugrf_vgrf_SOTA_mae',mae_error_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC5MzzWiSUGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0iXW42sifyi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV-ishNXSUGf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9lGzWRTSUGf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ode",
      "language": "python",
      "name": "ode"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bcef796ad1290c2772c71ce0232c4d4c4b45d4ccc2e51ef01ded1e839182c169"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}