{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-M_Nf2gGGa3",
        "outputId": "6b319154-5037-4e85-c84c-681a80c7281e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpBczXtk_9PV",
        "outputId": "90a29c25-e087-479f-8f9f-9e39b9eea5fa"
      },
      "source": [
        "cd /content/drive/MyDrive/gray_scott_results"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1JV7hS7EmJJsIJBj6IGG6rYBPHje96cuY/gray_scott_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIOLMnnR4QWx"
      },
      "source": [
        "# 1. 1D problem: the (time-independent) Burgers equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tJmIeTa_jw"
      },
      "source": [
        "## 1.1. MWT_1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlIPf1vWcpvk"
      },
      "source": [
        "from utils_3d import train, test, LpLoss, get_filter, UnitGaussianNormalizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GcZ4Ec_bU0r"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data_utils\n",
        "from typing import List, Tuple\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.io import loadmat, savemat\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import eval_legendre\n",
        "from sympy import Poly, legendre, Symbol\n",
        "import h5py\n",
        "\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "from timeit import default_timer\n",
        "from scipy.special import eval_legendre, gammaln"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu9qtnA-bDJN"
      },
      "source": [
        "# torch.manual_seed(0)\n",
        "# np.random.seed(0)\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adwy7KHQB9xY"
      },
      "source": [
        "def exp_pade_coeff(p, q):\n",
        "    J = np.arange(p)\n",
        "    log_num = gammaln(p+q-J+1) + gammaln(p+1) - gammaln(p+q+1) - gammaln(J+1) - gammaln(p-J+1)\n",
        "    num = np.exp(log_num)\n",
        "    num[0] = 1\n",
        "    J = np.arange(q)\n",
        "    log_dec = gammaln(p+q-J+1) + gammaln(q+1) - gammaln(p+q+1) - gammaln(J+1) - gammaln(q-J+1)\n",
        "    dec = np.exp(log_dec) * (-1)**(J)\n",
        "    dec[0] = 1\n",
        "    return num, dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn7yKvqgcKMA"
      },
      "source": [
        "def get_initializer(name):\n",
        "    \n",
        "    if name == 'xavier_normal':\n",
        "        init_ = partial(nn.init.xavier_normal_)\n",
        "    elif name == 'kaiming_uniform':\n",
        "        init_ = partial(nn.init.kaiming_uniform_)\n",
        "    elif name == 'kaiming_normal':\n",
        "        init_ = partial(nn.init.kaiming_normal_)\n",
        "    return init_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzysmfscb8AB"
      },
      "source": [
        "class sparseKernel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1, \n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernel,self).__init__()\n",
        "        \n",
        "        self.k = k\n",
        "        self.Li = nn.Linear(c*k, 128)\n",
        "        self.conv = self.convBlock(c*k, 128)\n",
        "#         self.Lo = nn.Linear(alpha*k, c*k)\n",
        "        self.Lo = nn.Linear(128, c*k)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        x = x.view(B, N, -1)\n",
        "#         x = F.relu(self.Li(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.Lo(x)\n",
        "        x = x.view(B, N, c, ich)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "        \n",
        "    def convBlock(self, ich, och):\n",
        "        net = nn.Sequential(\n",
        "            nn.Conv1d(ich, och, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "#             nn.Conv1d(och, och, 3, 1, 1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "        )\n",
        "        return net \n",
        "\n",
        "def compl_mul1d(x, weights):\n",
        "    # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "    return torch.einsum(\"bix,iox->box\", x, weights)\n",
        "\n",
        "\n",
        "class sparseKernelFT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k, alpha, c=1, \n",
        "                 nl = 1,\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(sparseKernelFT, self).__init__()        \n",
        "        \n",
        "        self.modes1 = alpha\n",
        "        self.scale = (1 / (c*k*c*k))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(c*k, c*k, self.modes1, dtype=torch.cfloat))\n",
        "        self.k = k\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, ck = x.shape # (B, N, c, k)\n",
        "        \n",
        "#         x = x.view(B, N, -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x_fft = torch.fft.rfft(x)\n",
        "        # Multiply relevant Fourier modes\n",
        "        l = min(self.modes1, N//2+1)\n",
        "        out_ft = torch.zeros(B, ck, N//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
        "        \n",
        "        out_ft[:, :, :l] = compl_mul1d(x_fft[:, :, :l], self.weights1[:, :, :l])\n",
        "        \n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft(out_ft, n=N)\n",
        "        x = x.permute(0, 2, 1)\n",
        "#         x = x.view(B, N, c, k)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class pade_exponential(nn.Module):\n",
        "    def __init__(self, \n",
        "                k, alpha, c=1,\n",
        "                p = 3, q = 4,\n",
        "                initializer = None,\n",
        "                **kwargs):\n",
        "        super(pade_exponential, self).__init__()\n",
        "        \n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        Pp, Pq = exp_pade_coeff(p, q)\n",
        "        \n",
        "        self.LinOperator = sparseKernelFT(k, alpha, c)\n",
        "        self.Linear = nn.Linear(c*k, c*k)\n",
        "        \n",
        "        self.register_buffer('Pp', torch.Tensor(Pp))\n",
        "        self.register_buffer('Pq', torch.Tensor(Pq))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, c, k = x.shape\n",
        "        \n",
        "        x = x.view(B, N, -1)\n",
        "        aggr_q = self.Pq[0] * x\n",
        "        for i in range(1, self.q):\n",
        "            x = self.LinOperator(x)\n",
        "            aggr_q += self.Pq[i] * x\n",
        "            \n",
        "        aggr_q = self.Linear(aggr_q)\n",
        "        aggr_q = F.relu(aggr_q)\n",
        "        \n",
        "        x = self.Pp[0] * aggr_q\n",
        "        for i in range(1, self.p):\n",
        "            aggr_q = self.LinOperator(aggr_q)\n",
        "            x += self.Pp[i] * aggr_q\n",
        "        \n",
        "        return x.view(B, N, c, k)\n",
        "\n",
        "    \n",
        "class MWT_CZ(nn.Module):\n",
        "    def __init__(self,\n",
        "                 k = 3, alpha = 5, \n",
        "                 L = 0, c = 1,\n",
        "                 p = 3, q = 4,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT_CZ, self).__init__()\n",
        "        \n",
        "        self.k = k\n",
        "        self.L = L\n",
        "        H0, H1, G0, G1, PHI0, PHI1 = get_filter(base, k)\n",
        "        H0r = H0@PHI0\n",
        "        G0r = G0@PHI0\n",
        "        H1r = H1@PHI1\n",
        "        G1r = G1@PHI1\n",
        "        \n",
        "        H0r[np.abs(H0r)<1e-8]=0\n",
        "        H1r[np.abs(H1r)<1e-8]=0\n",
        "        G0r[np.abs(G0r)<1e-8]=0\n",
        "        G1r[np.abs(G1r)<1e-8]=0\n",
        "        \n",
        "        self.A = pade_exponential(k, alpha, c, p, q)\n",
        "        self.B = pade_exponential(k, alpha, c, p, q)\n",
        "        self.C = pade_exponential(k, alpha, c, p, q)\n",
        "        \n",
        "        self.T0 = nn.Linear(k, k)\n",
        "\n",
        "        self.register_buffer('ec_s', torch.Tensor(\n",
        "            np.concatenate((H0.T, H1.T), axis=0)))\n",
        "        self.register_buffer('ec_d', torch.Tensor(\n",
        "            np.concatenate((G0.T, G1.T), axis=0)))\n",
        "        \n",
        "        self.register_buffer('rc_e', torch.Tensor(\n",
        "            np.concatenate((H0r, G0r), axis=0)))\n",
        "        self.register_buffer('rc_o', torch.Tensor(\n",
        "            np.concatenate((H1r, G1r), axis=0)))\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        B, N, c, ich = x.shape # (B, N, k)\n",
        "        ns = math.floor(np.log2(N))\n",
        "\n",
        "        Ud = torch.jit.annotate(List[Tensor], [])\n",
        "        Us = torch.jit.annotate(List[Tensor], [])\n",
        "#         decompose\n",
        "        for i in range(ns-self.L):\n",
        "            d, x = self.wavelet_transform(x)\n",
        "            Ud += [self.A(d) + self.B(x)]\n",
        "            Us += [self.C(d)]\n",
        "        x = self.T0(x) # coarsest scale transform\n",
        "\n",
        "#        reconstruct            \n",
        "        for i in range(ns-1-self.L,-1,-1):\n",
        "            x = x + Us[i]\n",
        "            x = torch.cat((x, Ud[i]), -1)\n",
        "            x = self.evenOdd(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "    def wavelet_transform(self, x):\n",
        "        xa = torch.cat([x[:, ::2, :, :], \n",
        "                        x[:, 1::2, :, :], \n",
        "                       ], -1)\n",
        "        d = torch.matmul(xa, self.ec_d)\n",
        "        s = torch.matmul(xa, self.ec_s)\n",
        "        return d, s\n",
        "        \n",
        "        \n",
        "    def evenOdd(self, x):\n",
        "        \n",
        "        B, N, c, ich = x.shape # (B, N, c, k)\n",
        "        assert ich == 2*self.k\n",
        "        x_e = torch.matmul(x, self.rc_e)\n",
        "        x_o = torch.matmul(x, self.rc_o)\n",
        "        \n",
        "        x = torch.zeros(B, N*2, c, self.k, \n",
        "            device = x.device)\n",
        "        x[..., ::2, :, :] = x_e\n",
        "        x[..., 1::2, :, :] = x_o\n",
        "        return x\n",
        "    \n",
        "    \n",
        "class MWT_exp(nn.Module):\n",
        "    def __init__(self,\n",
        "                 ich = 1, k = 3, alpha = 2, c = 1,\n",
        "                 p = 3, q = 4,\n",
        "                 nCZ = 3,\n",
        "                 L = 0,\n",
        "                 base = 'legendre',\n",
        "                 initializer = None,\n",
        "                 **kwargs):\n",
        "        super(MWT_exp,self).__init__()\n",
        "        \n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.L = L\n",
        "        self.nCZ = nCZ\n",
        "        self.Lk = nn.Linear(ich, c*k)\n",
        "        \n",
        "        self.MWT_CZ = nn.ModuleList(\n",
        "            [MWT_CZ(k, alpha, L, c, \n",
        "                p, q, base, \n",
        "                initializer) \n",
        "                for _ in range(nCZ)]\n",
        "        )\n",
        "        self.Lc0 = nn.Linear(c*k, 128)\n",
        "        self.Lc1 = nn.Linear(128, 1)\n",
        "        \n",
        "        if initializer is not None:\n",
        "            self.reset_parameters(initializer)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        B, N, ich = x.shape # (B, N, d)\n",
        "        ns = math.floor(np.log2(N))\n",
        "        x = self.Lk(x)\n",
        "        x = x.view(B, N, self.c, self.k)\n",
        "    \n",
        "        for i in range(self.nCZ):\n",
        "            x = self.MWT_CZ[i](x)\n",
        "#             \n",
        "            if i < self.nCZ-1:\n",
        "#                 x = torch.tanh(x)\n",
        "                x = F.relu(x)\n",
        "#             x = F.leaky_relu(x)\n",
        "\n",
        "        x = x.view(B, N, -1) # collapse c and k\n",
        "        x = self.Lc0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.Lc1(x)\n",
        "        return x.squeeze()\n",
        "    \n",
        "    def reset_parameters(self, initializer):\n",
        "        initializer(self.Lc0.weight)\n",
        "        initializer(self.Lc1.weight)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coupled Data\n",
        "\n",
        "ntrain = 1000\n",
        "ntest = 200\n",
        "\n",
        "sub = 2**0 #subsampling rate\n",
        "h = 2**8 // sub #total grid size divided by the subsampling rate\n",
        "s = h\n",
        "\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "rw_u = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Drho_t0_1.mat')\n",
        "x_data = rw_u['rho_t0'].astype(np.float32)\n",
        "y_data = rw_u['rho_t02'].astype(np.float32)\n",
        "print(x_data.shape)\n",
        "\n",
        "x_train_u = x_data[:ntrain,::sub]\n",
        "y_train_u = y_data[:ntrain,::sub]\n",
        "x_test_u = x_data[-ntest:,::sub]\n",
        "y_test_u = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_u = torch.from_numpy(x_train_u)\n",
        "x_test_u = torch.from_numpy(x_test_u)\n",
        "y_train_u = torch.from_numpy(y_train_u)\n",
        "y_test_u = torch.from_numpy(y_test_u)\n",
        "\n",
        "x_train_u = x_train_u.unsqueeze(-1)\n",
        "x_test_u = x_test_u.unsqueeze(-1)\n",
        "\n",
        "\n",
        "rw_v = loadmat('/content/drive/MyDrive/gray_scott_results/Coupled_PDE_data/kernel1Dphi_t0_1.mat')\n",
        "x_data = rw_v['phi_t0'].astype(np.float32)\n",
        "y_data = rw_v['phi_t02'].astype(np.float32)\n",
        "\n",
        "x_train_v = x_data[:ntrain,::sub]\n",
        "y_train_v = y_data[:ntrain,::sub]\n",
        "x_test_v = x_data[-ntest:,::sub]\n",
        "y_test_v = y_data[-ntest:,::sub]\n",
        "\n",
        "x_train_v = torch.from_numpy(x_train_v)\n",
        "x_test_v = torch.from_numpy(x_test_v)\n",
        "y_train_v = torch.from_numpy(y_train_v)\n",
        "y_test_v = torch.from_numpy(y_test_v)\n",
        "print(y_test_u.shape)\n",
        "\n",
        "x_train_v = x_train_v.unsqueeze(-1)\n",
        "x_test_v = x_test_v.unsqueeze(-1)\n",
        "\n",
        "x_train = torch.cat([x_train_u.reshape(ntrain,s,-1), x_train_v.reshape(ntrain,s,-1)], dim=1)\n",
        "x_test = torch.cat([x_test_u.reshape(ntest,s,-1), x_test_v.reshape(ntest,s,-1)], dim=1)\n",
        "\n",
        "y_train = torch.cat([y_train_u.reshape(ntrain,s,-1), y_train_v.reshape(ntrain,s,-1)], dim=1)\n",
        "y_test = torch.cat([y_test_u.reshape(ntest,s,-1), y_test_v.reshape(ntest,s,-1)], dim=1)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM4eRg2SDAoJ",
        "outputId": "82e85680-4313-47b3-edf1-e29f7b9a1c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 1024)\n",
            "torch.Size([200, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New model\n",
        "ich = 1\n",
        "initializer = get_initializer('xavier_normal') # xavier_normal, kaiming_normal, kaiming_uniform\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "model = MWT_exp(ich,\n",
        "            alpha = 10,\n",
        "            c = 4*4,\n",
        "            k = 4,\n",
        "            p = 4,\n",
        "            q = 2,\n",
        "            base = 'legendre',\n",
        "            nCZ = 1,\n",
        "            initializer = initializer,\n",
        "            ).to(device)\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 500\n",
        "step_size = 100\n",
        "gamma = 0.5"
      ],
      "metadata": {
        "id": "cOEDk6hgwWze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "maeloss = nn.L1Loss()\n",
        "myloss = LpLoss(size_average=False)\n",
        "\n",
        "error_u = []\n",
        "error_v = []\n",
        "\n",
        "mae_error_u = []\n",
        "mae_error_v = []\n",
        "\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_mse = 0\n",
        "    train_l2 = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
        "        # mse.backward()\n",
        "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
        "        l2.backward() # use the l2 relative loss\n",
        "\n",
        "        optimizer.step()\n",
        "        train_mse += mse.item()\n",
        "        train_l2 += l2.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    model.eval()\n",
        "    test_l2 = 0.0\n",
        "    test_l2_u = 0.0\n",
        "    test_l2_v = 0.0\n",
        "    test_mae = 0.0\n",
        "    test_mae_u = 0.0\n",
        "    test_mae_v = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "            out = model(x)\n",
        "            test_l2 += myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
        "            test_l2_u += myloss(out[:,:s].view(batch_size, -1), y[:,:s,:].view(batch_size, -1)).item()\n",
        "            test_l2_v += myloss(out[:,s:].view(batch_size, -1), y[:,s:,:].view(batch_size, -1)).item()\n",
        "            test_mae += maeloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
        "            test_mae_u += maeloss(out[:,:s].view(batch_size, -1), y[:,:s,:].view(batch_size, -1)).item()\n",
        "            test_mae_v += maeloss(out[:,s:].view(batch_size, -1), y[:,s:,:].view(batch_size, -1)).item()           \n",
        "\n",
        "    train_mse /= len(train_loader)\n",
        "    train_l2 /= ntrain\n",
        "    test_l2 /= ntest\n",
        "    test_l2_u /= ntest\n",
        "    test_l2_v /= ntest\n",
        "    test_mae /= ntest\n",
        "    test_mae_u /= ntest\n",
        "    test_mae_v /= ntest\n",
        "    error_u.append(test_l2_u)\n",
        "    error_v.append(test_l2_v)\n",
        "    mae_error_u.append(test_mae_u)\n",
        "    mae_error_v.append(test_mae_v)\n",
        "\n",
        "    t2 = default_timer()\n",
        "    print(ep, t2-t1, train_mse,test_mae, test_mae_u,test_mae_v,test_l2_u, test_l2_v)\n",
        "\n",
        "# np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_u_l1l1_ugrf_vgrf_Pade_cat_error',error_u)\n",
        "# np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_v_l1l1_ugrf_vgrf_Pade_cat_error',error_v)\n",
        "# np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_u_l1l1_ugrf_vgrf_Pade_cat_mae',mae_error_u)\n",
        "# np.save('/content/drive/MyDrive/gray_scott_results/Gray_scott_v_l1l1_ugrf_vgrf_Pade_cat_mae',mae_error_v)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t-D-LMR05LTV",
        "outputId": "b353fd59-3c62-4bd7-df68-59c6e04b1792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 12.05906741399997 0.015114312984514981 0.0014255493320524692 0.0014888045284897088 0.0013622941076755523 0.35833069801330564 0.16866528034210204\n",
            "1 7.417544961999965 0.0010060708702076227 0.00101664075627923 0.0010988858249038457 0.0009343957807868719 0.26447882890701296 0.11562112629413605\n",
            "2 7.3718223710000075 0.0010267835791455582 0.0008344144513830542 0.0008731253072619438 0.0007957036048173904 0.20807263135910034 0.09790749549865722\n",
            "3 7.429636240000036 0.0005076444527367129 0.00072191605810076 0.0007865834468975664 0.0006572486506775021 0.1838601279258728 0.07970373868942261\n",
            "4 7.696058691000076 0.00040474608656950297 0.0007785776536911726 0.0008521331660449505 0.0007050221553072334 0.20152315020561218 0.08526223719120025\n",
            "5 7.466100253999912 0.00045204564870800825 0.0008132560318335891 0.0007028093514963985 0.0009237027727067471 0.16053449988365173 0.10801330029964447\n",
            "6 7.413696809000044 0.0003574660752201453 0.0005773278605192899 0.0006149974325671792 0.0005396582977846265 0.1406325328350067 0.06385493397712708\n",
            "7 7.443805573000077 0.0003555561038956512 0.0005455522844567895 0.0005676943669095636 0.0005234102252870798 0.13112693309783935 0.061798847913742065\n",
            "8 7.399938507999991 0.00026402778894407675 0.0006504278536885976 0.000578099344857037 0.0007227563904598355 0.1342229986190796 0.08490232944488525\n",
            "9 7.47081796000009 0.00022487448746687732 0.00044481902150437234 0.00048250126652419565 0.00040713675087317825 0.11355020761489869 0.049832476675510405\n",
            "10 7.426411680000001 0.0001689067568804603 0.0004522798815742135 0.0005141566740348935 0.00039040309842675925 0.11985209703445435 0.04698794990777969\n",
            "11 7.37727527100003 0.00023129713415983132 0.0004949071956798435 0.0005066785216331482 0.00048313588835299017 0.11896737694740295 0.05983333468437195\n",
            "12 7.740646470999991 0.00014182628932758235 0.0003663618443533778 0.00039011382963508365 0.0003426098357886076 0.0910377049446106 0.040712999999523165\n",
            "13 7.399727134000045 0.00013783088536001742 0.00042737889103591444 0.0005017244629561901 0.0003530333028174937 0.11594199955463409 0.04310436755418778\n",
            "14 7.446087504999923 0.00015208506265480538 0.0010309161990880967 0.0009711126517504454 0.001090719709172845 0.21018303275108338 0.11634414374828339\n",
            "15 7.435844190000012 0.00014068237753235734 0.0003731650742702186 0.00035155847202986476 0.0003947716928087175 0.08256926834583282 0.04572993457317352\n",
            "16 7.378848919999882 0.00023473609566281083 0.0005106620397418737 0.0004145471635274589 0.0006067768903449178 0.09657613396644592 0.06796292066574097\n",
            "17 7.405488256000126 0.00010918211773969233 0.0003083408973179758 0.0003503193915821612 0.0002663624077104032 0.08325070381164551 0.032460656762123105\n",
            "18 7.342482239999981 8.606612267612945e-05 0.00033632103120908143 0.0003855467215180397 0.00028709532925859095 0.08909494757652282 0.03430036693811417\n",
            "19 7.44361931200001 7.895661816291977e-05 0.00033447349909693 0.0003961564553901553 0.0002727905358187854 0.09146899580955506 0.03296105235815048\n",
            "20 7.659282569999959 8.099832019070163e-05 0.0003030299162492156 0.00032068785978481176 0.0002853719727136195 0.07548615097999573 0.03460212558507919\n",
            "21 7.3674090299998625 8.474760405078996e-05 0.00032941226148977873 0.0004232532484456897 0.0002355712791904807 0.0977736085653305 0.02858746990561485\n",
            "22 7.370904681999946 7.320777549466584e-05 0.000340096156578511 0.000443781903013587 0.0002364103961735964 0.1015778112411499 0.028991982340812683\n",
            "23 7.375504568999986 7.652248918020632e-05 0.00030470649944618343 0.0003263692557811737 0.0002830437454394996 0.077054123878479 0.03455995857715607\n",
            "24 7.393900961000099 8.112614435958677e-05 0.00034775350941345095 0.00041589870816096666 0.0002796083246357739 0.09559734225273132 0.03336875557899475\n",
            "25 7.476869954999984 0.00013423819691524841 0.0005672776931896806 0.0003978856117464602 0.0007366697862744332 0.08936891734600066 0.08503552675247192\n",
            "26 7.3211041219999515 0.00010453887465700972 0.0002714447560720146 0.00028211401076987385 0.00026077549438923597 0.06585413157939911 0.032337762117385864\n",
            "27 7.4004891960000805 5.970663121843245e-05 0.0002641401067376137 0.00027728171087801454 0.00025099850492551925 0.06430762529373169 0.03118204414844513\n",
            "28 7.547665643000073 5.777770114946179e-05 0.00031171677866950634 0.00027373278047889473 0.00034970078151673077 0.06426466703414917 0.04147348016500473\n",
            "29 7.607517865000091 0.00010424378517200239 0.0006367868650704622 0.00038299527252092956 0.0008905784040689469 0.0898603492975235 0.10029948592185974\n",
            "30 7.452929657000141 8.916396145650652e-05 0.0006574834417551756 0.00047728067729622123 0.0008376861829310655 0.1066002917289734 0.08912162959575654\n",
            "31 7.473255641000151 0.00010507369763217866 0.0002636354439891875 0.0002930928417481482 0.0002341780415736139 0.06932098865509033 0.028259764313697814\n",
            "32 7.418703906000019 6.732534377078991e-05 0.00043458342086523773 0.00027745419181883334 0.0005917126359418034 0.0645430713891983 0.06524051904678345\n",
            "33 7.3743569820001085 7.526563400460873e-05 0.00027724183164536955 0.00030648585641756653 0.0002479978115297854 0.07161303520202637 0.029689099490642547\n",
            "34 7.35225080500004 6.302902587776771e-05 0.0002656115731224418 0.00027938223676756026 0.0002518409164622426 0.06498960614204406 0.029999899715185165\n",
            "35 7.397309457000119 4.7385883626702706e-05 0.0002356831612996757 0.0002565651177428663 0.00021480121184140445 0.059722782969474794 0.025370092242956162\n",
            "36 7.351931629999854 4.24113827466499e-05 0.0002194502460770309 0.0002455847687087953 0.00019331570714712143 0.05705735981464386 0.023505253195762636\n",
            "37 7.763426199999913 6.601842262170975e-05 0.00035793249495327475 0.000320476817432791 0.0003953881561756134 0.07520629346370697 0.04649703323841095\n",
            "38 7.409716435000064 6.574524773895973e-05 0.0003252440667711198 0.0003073061699979007 0.00034318195655941964 0.07039672672748566 0.03974476486444473\n",
            "39 7.390158328000098 5.634980418108171e-05 0.00023320335429161788 0.00025827877689152954 0.00020812792470678687 0.06050739169120788 0.025075047463178634\n",
            "40 7.399550374 4.7857167191978076e-05 0.00022001715027727186 0.00025050704600289466 0.0001895272498950362 0.05862339735031128 0.023136942833662032\n",
            "41 7.485984844000086 4.196800327918027e-05 0.00025232939049601554 0.00026546503417193887 0.00023919375147670507 0.061268115639686586 0.02977318525314331\n",
            "42 7.443746534999946 4.5945708516228477e-05 0.00022686826763674618 0.00023617340018972755 0.00021756314672529697 0.054433901011943814 0.025971733778715134\n",
            "43 7.393237265999915 3.804459156526718e-05 0.00022142797708511354 0.00024670670041814446 0.00019614925142377615 0.05749900728464127 0.024210401326417923\n",
            "44 7.399389780000092 6.194376124767587e-05 0.00039255026960745455 0.000357699238229543 0.00042740128934383394 0.08151017487049103 0.047654045224189756\n",
            "45 7.650873566999962 4.8198761651292446e-05 0.0002275925246067345 0.0002268183301202953 0.00022836671909317373 0.05249856472015381 0.02705851525068283\n",
            "46 7.358289528000114 4.6603771006630266e-05 0.0005861621396616102 0.0003377863811329007 0.0008345379121601582 0.07641338288784028 0.08814890801906586\n",
            "47 7.375788335999914 8.04530087771127e-05 0.0003475793800316751 0.0003697179351001978 0.0003254408296197653 0.08237371742725372 0.03714133679866791\n",
            "48 7.36225659500019 6.274877556279534e-05 0.00040374378208070995 0.000297915181145072 0.0005095723737031222 0.0693797892332077 0.05630523830652237\n",
            "49 7.398456696000039 5.4981732464511876e-05 0.00034699188079684973 0.00046496860682964324 0.00022901513148099184 0.10312036216259003 0.02863559529185295\n",
            "50 7.356880176999994 3.6614849632314874e-05 0.0001935384259559214 0.00021059815888293087 0.00017647870117798448 0.049029060900211335 0.021437438577413558\n",
            "51 7.256072825999809 3.1243422672559976e-05 0.00019812715705484153 0.00021762411925010383 0.0001786302076652646 0.04972287625074387 0.022005719244480134\n",
            "52 7.454250029999912 3.935836099117296e-05 0.00023422273341566324 0.00025510876439511777 0.00021333669777959586 0.057992516458034514 0.025724812895059585\n",
            "53 7.778308372999845 6.015830858814297e-05 0.0004185582557693124 0.0003362015029415488 0.0005009149992838502 0.07423011720180511 0.05753799021244049\n",
            "54 7.414900968999973 4.9868598180182746e-05 0.00021651679184287787 0.00023431892273947596 0.00019871466560289264 0.05468389242887497 0.023932999670505523\n",
            "55 7.546476733999953 3.213170586604974e-05 0.0002458752342499793 0.0002622585790231824 0.00022949189646169542 0.05940800607204437 0.027854048162698746\n",
            "56 7.395881370999859 4.623094602720812e-05 0.00037373193306848405 0.0003068212722428143 0.0004406425869092345 0.06954031169414521 0.04903937131166458\n",
            "57 7.39843222699983 6.05567880847957e-05 0.00045183152891695497 0.0003079440328292549 0.0005957190366461873 0.07037163019180298 0.06477777361869812\n",
            "58 7.3532682210000075 4.599134164891439e-05 0.00020256201038137077 0.0002143911924213171 0.00019073282135650515 0.04948314279317856 0.023109778016805648\n",
            "59 7.444328614000142 3.1979698360373734e-05 0.0001899988390505314 0.00021313968929462135 0.00016685797716490926 0.05052869617938995 0.020874288380146027\n",
            "60 7.373781516999998 3.769573901081458e-05 0.00025599339278414845 0.00026443920796737075 0.00024754755897447465 0.06131857633590698 0.028506805300712586\n",
            "61 7.651104808999889 4.407208594784606e-05 0.0001959680439904332 0.00022082770010456443 0.00017110840184614062 0.05235498785972595 0.021065354198217392\n",
            "62 7.360412989999986 3.09856377862161e-05 0.00020410081138834359 0.00022831536130979657 0.00017988626146689057 0.0528222930431366 0.021846112310886383\n",
            "63 7.379864559999987 4.442231416760478e-05 0.00025257562985643746 0.00020331262960098683 0.0003018386196345091 0.047920916378498074 0.03567560344934464\n",
            "64 7.423188646999961 4.507230401941342e-05 0.0004002340673469007 0.00029667135095223784 0.0005037967767566443 0.06613500654697418 0.05557563096284866\n",
            "65 7.3815323419999 3.885615531544317e-05 0.00021426477236673235 0.00023212902247905733 0.00019640051876194776 0.054103282690048216 0.023846483528614043\n",
            "66 7.3878467130000445 2.4607246959931217e-05 0.00018346717930398882 0.00022013978217728437 0.00014679457410238684 0.050343662798404694 0.01798732042312622\n",
            "67 7.390490145000058 2.800536263748654e-05 0.00017523787333630027 0.00020639796392060816 0.0001440777664538473 0.04781303733587265 0.017772332578897477\n",
            "68 7.3158762429998205 3.0542257518391126e-05 0.0003029358643107116 0.00024138967972248793 0.00036448205122724175 0.0547935825586319 0.039628272652626036\n",
            "69 7.6309417850000045 3.805873675446492e-05 0.00020886222715489568 0.00027236493537202477 0.0001453595154453069 0.06318163067102432 0.018065166473388673\n",
            "70 7.399773470000127 2.9148234916647197e-05 0.0001970591233111918 0.0002320163301192224 0.00016210192814469337 0.053352846801280975 0.01971642941236496\n",
            "71 7.380794862999892 2.2352897030941676e-05 0.00016902503091841936 0.0001911767350975424 0.00014687332208268344 0.043532393872737885 0.01799916461110115\n",
            "72 7.338690590999931 2.7115657594549703e-05 0.00017286456073634327 0.0001826072484254837 0.00016312186722643673 0.043257173299789425 0.01969505026936531\n",
            "73 7.305299585999819 2.28949910706433e-05 0.0001737424556631595 0.00018086674972437323 0.000166618162766099 0.042670160830020905 0.020499007999897\n",
            "74 7.376599366000164 2.2377270324795974e-05 0.00017998777446337044 0.00018210972310043873 0.00017786581651307642 0.04263426631689072 0.02205969214439392\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c800441a38ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# mse.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-c636a5c873f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnCZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMWT_CZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnCZ\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-c636a5c873f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavelet_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mUd\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mUs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# coarsest scale transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-c636a5c873f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0maggr_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maggr_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0maggr_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinOperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3aCOnqp_Oit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}